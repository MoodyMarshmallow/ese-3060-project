====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.warmdown_iters = int(os.environ.get("WARMDOWN_ITERS", args.warmdown_iters))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: c357df511c00be06ac81976d70129bbee5b60c5d
seed: 1337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 1500,
  "learning_rate": 0.00468,
  "warmup_iters": 0,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 1337,
  "attn_gate": "elementwise",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 0,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Mon Dec  8 03:38:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   46C    P0            145W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            137W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0            108W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   45C    P0            132W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   48C    P0            115W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   46C    P0            129W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   47C    P0            118W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   46C    P0            139W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1500 val_loss:16.0015 train_time:248ms step_avg:nanms
step:1/1500 train_loss:16.0007 train_time:75449ms step_avg:nanms
step:2/1500 train_loss:9.5957 train_time:76415ms step_avg:nanms
step:3/1500 train_loss:8.5035 train_time:76830ms step_avg:nanms
step:4/1500 train_loss:7.8447 train_time:77244ms step_avg:nanms
step:5/1500 train_loss:7.5750 train_time:77657ms step_avg:nanms
step:6/1500 train_loss:7.5815 train_time:78071ms step_avg:nanms
step:7/1500 train_loss:7.0295 train_time:78487ms step_avg:nanms
step:8/1500 train_loss:7.3492 train_time:78903ms step_avg:nanms
step:9/1500 train_loss:7.0494 train_time:79319ms step_avg:nanms
step:10/1500 train_loss:6.9080 train_time:79734ms step_avg:nanms
step:11/1500 train_loss:6.7969 train_time:401ms step_avg:nanms
step:12/1500 train_loss:6.7345 train_time:817ms step_avg:nanms
step:13/1500 train_loss:6.5312 train_time:1232ms step_avg:410.56ms
step:14/1500 train_loss:6.5098 train_time:1647ms step_avg:411.86ms
step:15/1500 train_loss:6.4812 train_time:2063ms step_avg:412.69ms
step:16/1500 train_loss:6.4173 train_time:2480ms step_avg:413.37ms
step:17/1500 train_loss:6.4315 train_time:2896ms step_avg:413.75ms
step:18/1500 train_loss:6.4715 train_time:3312ms step_avg:414.00ms
step:19/1500 train_loss:6.2872 train_time:3729ms step_avg:414.32ms
step:20/1500 train_loss:6.3332 train_time:4144ms step_avg:414.41ms
step:21/1500 train_loss:5.9959 train_time:4561ms step_avg:414.63ms
step:22/1500 train_loss:6.3528 train_time:4976ms step_avg:414.69ms
step:23/1500 train_loss:6.5603 train_time:5391ms step_avg:414.66ms
step:24/1500 train_loss:6.2503 train_time:5806ms step_avg:414.73ms
step:25/1500 train_loss:6.4028 train_time:6222ms step_avg:414.81ms
step:26/1500 train_loss:6.1058 train_time:6639ms step_avg:414.92ms
step:27/1500 train_loss:6.0172 train_time:7055ms step_avg:414.98ms
step:28/1500 train_loss:6.1719 train_time:7470ms step_avg:415.02ms
step:29/1500 train_loss:5.8413 train_time:7887ms step_avg:415.08ms
step:30/1500 train_loss:6.1241 train_time:8302ms step_avg:415.09ms
step:31/1500 train_loss:5.9523 train_time:8717ms step_avg:415.12ms
step:32/1500 train_loss:5.9289 train_time:9134ms step_avg:415.19ms
step:33/1500 train_loss:5.7621 train_time:9552ms step_avg:415.31ms
step:34/1500 train_loss:6.0474 train_time:9967ms step_avg:415.31ms
step:35/1500 train_loss:5.9661 train_time:10384ms step_avg:415.34ms
step:36/1500 train_loss:6.1225 train_time:10801ms step_avg:415.40ms
step:37/1500 train_loss:6.0489 train_time:11216ms step_avg:415.40ms
step:38/1500 train_loss:5.9508 train_time:11632ms step_avg:415.43ms
step:39/1500 train_loss:5.8354 train_time:12048ms step_avg:415.46ms
step:40/1500 train_loss:5.8468 train_time:12464ms step_avg:415.48ms
step:41/1500 train_loss:5.7670 train_time:12881ms step_avg:415.53ms
step:42/1500 train_loss:5.7825 train_time:13297ms step_avg:415.53ms
step:43/1500 train_loss:5.6678 train_time:13713ms step_avg:415.55ms
step:44/1500 train_loss:5.7748 train_time:14131ms step_avg:415.60ms
step:45/1500 train_loss:5.7386 train_time:14547ms step_avg:415.63ms
step:46/1500 train_loss:5.8996 train_time:14964ms step_avg:415.67ms
step:47/1500 train_loss:5.6797 train_time:15380ms step_avg:415.68ms
step:48/1500 train_loss:5.5616 train_time:15797ms step_avg:415.70ms
step:49/1500 train_loss:5.7697 train_time:16213ms step_avg:415.73ms
step:50/1500 train_loss:5.6614 train_time:16629ms step_avg:415.71ms
step:51/1500 train_loss:5.7833 train_time:17047ms step_avg:415.79ms
step:52/1500 train_loss:5.6496 train_time:17464ms step_avg:415.81ms
step:53/1500 train_loss:5.5171 train_time:17880ms step_avg:415.82ms
step:54/1500 train_loss:5.6627 train_time:18297ms step_avg:415.84ms
step:55/1500 train_loss:5.5298 train_time:18713ms step_avg:415.85ms
step:56/1500 train_loss:5.8807 train_time:19130ms step_avg:415.88ms
step:57/1500 train_loss:5.5305 train_time:19546ms step_avg:415.87ms
step:58/1500 train_loss:5.3966 train_time:19963ms step_avg:415.89ms
step:59/1500 train_loss:5.5373 train_time:20382ms step_avg:415.95ms
step:60/1500 train_loss:5.5114 train_time:20797ms step_avg:415.95ms
step:61/1500 train_loss:5.6161 train_time:21215ms step_avg:415.98ms
step:62/1500 train_loss:5.3748 train_time:21633ms step_avg:416.02ms
step:63/1500 train_loss:5.4836 train_time:22051ms step_avg:416.06ms
step:64/1500 train_loss:5.4604 train_time:22467ms step_avg:416.06ms
step:65/1500 train_loss:5.1100 train_time:22884ms step_avg:416.06ms
step:66/1500 train_loss:5.2895 train_time:23301ms step_avg:416.08ms
step:67/1500 train_loss:5.4381 train_time:23718ms step_avg:416.11ms
step:68/1500 train_loss:5.2998 train_time:24135ms step_avg:416.11ms
step:69/1500 train_loss:5.5696 train_time:24556ms step_avg:416.20ms
step:70/1500 train_loss:5.2047 train_time:24974ms step_avg:416.23ms
step:71/1500 train_loss:5.2506 train_time:25389ms step_avg:416.22ms
step:72/1500 train_loss:5.4468 train_time:25805ms step_avg:416.22ms
step:73/1500 train_loss:5.3867 train_time:26222ms step_avg:416.22ms
step:74/1500 train_loss:5.2510 train_time:26639ms step_avg:416.23ms
step:75/1500 train_loss:5.3839 train_time:27058ms step_avg:416.27ms
step:76/1500 train_loss:5.3555 train_time:27475ms step_avg:416.29ms
step:77/1500 train_loss:5.3130 train_time:27891ms step_avg:416.29ms
step:78/1500 train_loss:5.3969 train_time:28309ms step_avg:416.31ms
step:79/1500 train_loss:5.4880 train_time:28725ms step_avg:416.30ms
step:80/1500 train_loss:5.2645 train_time:29141ms step_avg:416.30ms
step:81/1500 train_loss:5.3677 train_time:29559ms step_avg:416.33ms
step:82/1500 train_loss:5.1316 train_time:29976ms step_avg:416.33ms
step:83/1500 train_loss:5.3128 train_time:30393ms step_avg:416.34ms
step:84/1500 train_loss:5.2646 train_time:30810ms step_avg:416.35ms
step:85/1500 train_loss:5.2435 train_time:31226ms step_avg:416.35ms
step:86/1500 train_loss:5.0927 train_time:31642ms step_avg:416.35ms
step:87/1500 train_loss:5.3110 train_time:32060ms step_avg:416.37ms
step:88/1500 train_loss:5.2165 train_time:32478ms step_avg:416.38ms
step:89/1500 train_loss:5.2784 train_time:32896ms step_avg:416.40ms
step:90/1500 train_loss:5.2252 train_time:33313ms step_avg:416.42ms
step:91/1500 train_loss:5.1616 train_time:33730ms step_avg:416.42ms
step:92/1500 train_loss:5.1409 train_time:34147ms step_avg:416.43ms
step:93/1500 train_loss:5.2956 train_time:34564ms step_avg:416.44ms
step:94/1500 train_loss:5.1025 train_time:34981ms step_avg:416.44ms
step:95/1500 train_loss:5.1134 train_time:35397ms step_avg:416.44ms
step:96/1500 train_loss:5.1616 train_time:35814ms step_avg:416.44ms
step:97/1500 train_loss:5.0609 train_time:36232ms step_avg:416.45ms
step:98/1500 train_loss:5.1458 train_time:36649ms step_avg:416.47ms
step:99/1500 train_loss:5.0736 train_time:37065ms step_avg:416.47ms
step:100/1500 train_loss:5.1989 train_time:37482ms step_avg:416.46ms
step:101/1500 train_loss:5.1630 train_time:37898ms step_avg:416.47ms
step:102/1500 train_loss:5.0659 train_time:38316ms step_avg:416.48ms
step:103/1500 train_loss:5.1645 train_time:38733ms step_avg:416.48ms
step:104/1500 train_loss:5.1078 train_time:39153ms step_avg:416.52ms
step:105/1500 train_loss:4.9787 train_time:39570ms step_avg:416.52ms
step:106/1500 train_loss:5.0812 train_time:39986ms step_avg:416.52ms
step:107/1500 train_loss:5.2737 train_time:40403ms step_avg:416.53ms
step:108/1500 train_loss:5.0380 train_time:40820ms step_avg:416.54ms
step:109/1500 train_loss:4.8381 train_time:41239ms step_avg:416.56ms
step:110/1500 train_loss:5.0195 train_time:41657ms step_avg:416.57ms
step:111/1500 train_loss:4.9952 train_time:42079ms step_avg:416.62ms
step:112/1500 train_loss:4.9637 train_time:42497ms step_avg:416.64ms
step:113/1500 train_loss:5.0769 train_time:42915ms step_avg:416.65ms
step:114/1500 train_loss:5.0059 train_time:43331ms step_avg:416.65ms
step:115/1500 train_loss:4.8501 train_time:43748ms step_avg:416.65ms
step:116/1500 train_loss:5.0087 train_time:44165ms step_avg:416.65ms
step:117/1500 train_loss:4.9267 train_time:44581ms step_avg:416.65ms
step:118/1500 train_loss:4.8783 train_time:44999ms step_avg:416.66ms
step:119/1500 train_loss:5.0292 train_time:45415ms step_avg:416.66ms
step:120/1500 train_loss:4.9733 train_time:45830ms step_avg:416.64ms
step:121/1500 train_loss:4.9215 train_time:46248ms step_avg:416.65ms
step:122/1500 train_loss:4.8043 train_time:46664ms step_avg:416.64ms
step:123/1500 train_loss:4.9314 train_time:47080ms step_avg:416.64ms
step:124/1500 train_loss:4.7806 train_time:47496ms step_avg:416.64ms
step:125/1500 train_loss:5.0952 train_time:47912ms step_avg:416.63ms
step:125/1500 val_loss:4.9271 train_time:47926ms step_avg:416.75ms
step:126/1500 train_loss:4.9743 train_time:48333ms step_avg:416.66ms
step:127/1500 train_loss:4.9242 train_time:48750ms step_avg:416.67ms
step:128/1500 train_loss:4.9775 train_time:49167ms step_avg:416.67ms
step:129/1500 train_loss:4.8443 train_time:49585ms step_avg:416.68ms
step:130/1500 train_loss:5.1660 train_time:49999ms step_avg:416.66ms
step:131/1500 train_loss:4.9134 train_time:50416ms step_avg:416.66ms
step:132/1500 train_loss:4.9209 train_time:50832ms step_avg:416.65ms
step:133/1500 train_loss:4.8666 train_time:51249ms step_avg:416.66ms
step:134/1500 train_loss:4.9147 train_time:51666ms step_avg:416.66ms
step:135/1500 train_loss:4.8039 train_time:52083ms step_avg:416.66ms
step:136/1500 train_loss:4.9333 train_time:52500ms step_avg:416.67ms
step:137/1500 train_loss:4.6985 train_time:52924ms step_avg:416.72ms
step:138/1500 train_loss:4.8654 train_time:53341ms step_avg:416.72ms
step:139/1500 train_loss:4.8174 train_time:53757ms step_avg:416.72ms
step:140/1500 train_loss:4.8509 train_time:54174ms step_avg:416.72ms
step:141/1500 train_loss:4.9167 train_time:54592ms step_avg:416.73ms
step:142/1500 train_loss:4.7813 train_time:55007ms step_avg:416.72ms
step:143/1500 train_loss:4.8393 train_time:55426ms step_avg:416.73ms
step:144/1500 train_loss:4.7095 train_time:55842ms step_avg:416.73ms
step:145/1500 train_loss:4.8406 train_time:56260ms step_avg:416.74ms
step:146/1500 train_loss:4.7953 train_time:56677ms step_avg:416.74ms
step:147/1500 train_loss:4.6603 train_time:57093ms step_avg:416.74ms
step:148/1500 train_loss:4.8151 train_time:57510ms step_avg:416.74ms
step:149/1500 train_loss:4.8148 train_time:57927ms step_avg:416.74ms
step:150/1500 train_loss:4.8354 train_time:58344ms step_avg:416.75ms
step:151/1500 train_loss:4.8839 train_time:58762ms step_avg:416.75ms
step:152/1500 train_loss:4.7689 train_time:59180ms step_avg:416.76ms
step:153/1500 train_loss:4.7653 train_time:59598ms step_avg:416.77ms
step:154/1500 train_loss:4.8545 train_time:60016ms step_avg:416.78ms
step:155/1500 train_loss:4.8110 train_time:60431ms step_avg:416.77ms
step:156/1500 train_loss:4.7659 train_time:60847ms step_avg:416.76ms
step:157/1500 train_loss:4.7897 train_time:61264ms step_avg:416.76ms
step:158/1500 train_loss:4.8998 train_time:61682ms step_avg:416.77ms
step:159/1500 train_loss:4.6943 train_time:62099ms step_avg:416.77ms
step:160/1500 train_loss:4.7712 train_time:62516ms step_avg:416.77ms
step:161/1500 train_loss:4.5991 train_time:62936ms step_avg:416.79ms
step:162/1500 train_loss:4.7822 train_time:63353ms step_avg:416.80ms
step:163/1500 train_loss:4.8224 train_time:63771ms step_avg:416.80ms
step:164/1500 train_loss:4.8036 train_time:64186ms step_avg:416.79ms
step:165/1500 train_loss:4.6115 train_time:64604ms step_avg:416.80ms
step:166/1500 train_loss:4.7386 train_time:65024ms step_avg:416.82ms
step:167/1500 train_loss:4.8724 train_time:65440ms step_avg:416.81ms
step:168/1500 train_loss:4.6599 train_time:65857ms step_avg:416.81ms
step:169/1500 train_loss:4.7634 train_time:66274ms step_avg:416.82ms
step:170/1500 train_loss:4.6104 train_time:66691ms step_avg:416.82ms
step:171/1500 train_loss:4.5105 train_time:67107ms step_avg:416.81ms
step:172/1500 train_loss:4.6686 train_time:67526ms step_avg:416.83ms
step:173/1500 train_loss:4.6536 train_time:67943ms step_avg:416.83ms
step:174/1500 train_loss:4.7042 train_time:68360ms step_avg:416.83ms
step:175/1500 train_loss:4.8572 train_time:68777ms step_avg:416.83ms
step:176/1500 train_loss:4.7050 train_time:69193ms step_avg:416.83ms
step:177/1500 train_loss:4.5597 train_time:69611ms step_avg:416.83ms
step:178/1500 train_loss:4.5262 train_time:70028ms step_avg:416.83ms
step:179/1500 train_loss:4.6008 train_time:70445ms step_avg:416.83ms
step:180/1500 train_loss:4.6068 train_time:70863ms step_avg:416.84ms
step:181/1500 train_loss:4.6029 train_time:71278ms step_avg:416.83ms
step:182/1500 train_loss:4.7387 train_time:71695ms step_avg:416.83ms
step:183/1500 train_loss:4.6068 train_time:72111ms step_avg:416.83ms
step:184/1500 train_loss:4.5539 train_time:72529ms step_avg:416.83ms
step:185/1500 train_loss:4.5657 train_time:72945ms step_avg:416.83ms
step:186/1500 train_loss:4.6809 train_time:73361ms step_avg:416.83ms
step:187/1500 train_loss:4.5981 train_time:73779ms step_avg:416.83ms
step:188/1500 train_loss:4.7967 train_time:74196ms step_avg:416.83ms
step:189/1500 train_loss:4.6103 train_time:75505ms step_avg:421.81ms
step:190/1500 train_loss:4.5253 train_time:76110ms step_avg:422.83ms
step:191/1500 train_loss:4.6738 train_time:76539ms step_avg:422.87ms
step:192/1500 train_loss:4.5195 train_time:76966ms step_avg:422.89ms
step:193/1500 train_loss:4.4470 train_time:77391ms step_avg:422.90ms
step:194/1500 train_loss:4.6824 train_time:77818ms step_avg:422.92ms
step:195/1500 train_loss:4.5992 train_time:78241ms step_avg:422.92ms
step:196/1500 train_loss:4.7825 train_time:78667ms step_avg:422.94ms
step:197/1500 train_loss:4.6462 train_time:79092ms step_avg:422.95ms
step:198/1500 train_loss:4.4956 train_time:79516ms step_avg:422.96ms
step:199/1500 train_loss:4.5710 train_time:79940ms step_avg:422.96ms
step:200/1500 train_loss:4.4458 train_time:80366ms step_avg:422.98ms
step:201/1500 train_loss:4.5290 train_time:80789ms step_avg:422.98ms
step:202/1500 train_loss:4.4321 train_time:81215ms step_avg:422.99ms
step:203/1500 train_loss:4.6831 train_time:81640ms step_avg:423.01ms
step:204/1500 train_loss:4.5446 train_time:82063ms step_avg:423.01ms
step:205/1500 train_loss:4.5778 train_time:82486ms step_avg:423.01ms
step:206/1500 train_loss:4.6796 train_time:82913ms step_avg:423.02ms
step:207/1500 train_loss:4.3448 train_time:83339ms step_avg:423.04ms
step:208/1500 train_loss:4.5054 train_time:83762ms step_avg:423.04ms
step:209/1500 train_loss:4.4848 train_time:84184ms step_avg:423.04ms
step:210/1500 train_loss:4.6431 train_time:84609ms step_avg:423.04ms
step:211/1500 train_loss:4.5640 train_time:85034ms step_avg:423.06ms
step:212/1500 train_loss:4.4427 train_time:85459ms step_avg:423.06ms
step:213/1500 train_loss:4.5590 train_time:85884ms step_avg:423.07ms
step:214/1500 train_loss:4.4131 train_time:86309ms step_avg:423.08ms
step:215/1500 train_loss:4.4867 train_time:86735ms step_avg:423.10ms
step:216/1500 train_loss:4.3482 train_time:87159ms step_avg:423.10ms
step:217/1500 train_loss:4.4474 train_time:87584ms step_avg:423.11ms
step:218/1500 train_loss:4.4068 train_time:88008ms step_avg:423.12ms
step:219/1500 train_loss:4.4372 train_time:88432ms step_avg:423.12ms
step:220/1500 train_loss:4.4273 train_time:88855ms step_avg:423.12ms
step:221/1500 train_loss:4.4662 train_time:89278ms step_avg:423.12ms
step:222/1500 train_loss:4.4822 train_time:89703ms step_avg:423.13ms
step:223/1500 train_loss:4.4034 train_time:90126ms step_avg:423.13ms
step:224/1500 train_loss:4.4079 train_time:90550ms step_avg:423.13ms
step:225/1500 train_loss:4.6124 train_time:90973ms step_avg:423.13ms
step:226/1500 train_loss:4.2659 train_time:91395ms step_avg:423.12ms
step:227/1500 train_loss:4.3326 train_time:91817ms step_avg:423.12ms
step:228/1500 train_loss:4.3387 train_time:92241ms step_avg:423.12ms
step:229/1500 train_loss:4.4869 train_time:92664ms step_avg:423.12ms
step:230/1500 train_loss:4.2822 train_time:93088ms step_avg:423.13ms
step:231/1500 train_loss:4.4165 train_time:93511ms step_avg:423.13ms
step:232/1500 train_loss:4.2848 train_time:93936ms step_avg:423.14ms
step:233/1500 train_loss:4.2999 train_time:94363ms step_avg:423.15ms
step:234/1500 train_loss:4.4668 train_time:94787ms step_avg:423.16ms
step:235/1500 train_loss:4.3473 train_time:95211ms step_avg:423.16ms
step:236/1500 train_loss:4.2453 train_time:95634ms step_avg:423.16ms
step:237/1500 train_loss:4.4429 train_time:96058ms step_avg:423.16ms
step:238/1500 train_loss:4.4070 train_time:96482ms step_avg:423.16ms
step:239/1500 train_loss:4.2732 train_time:96906ms step_avg:423.17ms
step:240/1500 train_loss:4.4300 train_time:97331ms step_avg:423.18ms
step:241/1500 train_loss:4.4280 train_time:97755ms step_avg:423.18ms
step:242/1500 train_loss:4.3020 train_time:98179ms step_avg:423.19ms
step:243/1500 train_loss:4.4901 train_time:98602ms step_avg:423.19ms
step:244/1500 train_loss:4.3292 train_time:99028ms step_avg:423.20ms
step:245/1500 train_loss:4.3528 train_time:99451ms step_avg:423.19ms
step:246/1500 train_loss:4.4395 train_time:99875ms step_avg:423.20ms
step:247/1500 train_loss:4.3771 train_time:100297ms step_avg:423.19ms
step:248/1500 train_loss:4.3118 train_time:100720ms step_avg:423.19ms
step:249/1500 train_loss:4.4358 train_time:101143ms step_avg:423.19ms
step:250/1500 train_loss:4.2207 train_time:101570ms step_avg:423.21ms
step:250/1500 val_loss:4.3157 train_time:101583ms step_avg:423.26ms
step:251/1500 train_loss:4.2719 train_time:101991ms step_avg:423.20ms
step:252/1500 train_loss:4.3852 train_time:102415ms step_avg:423.20ms
step:253/1500 train_loss:4.4163 train_time:102838ms step_avg:423.20ms
step:254/1500 train_loss:4.2456 train_time:103262ms step_avg:423.21ms
step:255/1500 train_loss:4.1915 train_time:103686ms step_avg:423.21ms
step:256/1500 train_loss:4.3649 train_time:104109ms step_avg:423.21ms
step:257/1500 train_loss:4.2814 train_time:104534ms step_avg:423.21ms
step:258/1500 train_loss:4.2975 train_time:104957ms step_avg:423.21ms
step:259/1500 train_loss:4.2604 train_time:105381ms step_avg:423.22ms
step:260/1500 train_loss:4.2936 train_time:105806ms step_avg:423.22ms
step:261/1500 train_loss:4.3502 train_time:106230ms step_avg:423.23ms
step:262/1500 train_loss:4.2988 train_time:106653ms step_avg:423.23ms
step:263/1500 train_loss:4.2684 train_time:107079ms step_avg:423.24ms
step:264/1500 train_loss:4.1793 train_time:107502ms step_avg:423.23ms
step:265/1500 train_loss:4.2628 train_time:107926ms step_avg:423.24ms
step:266/1500 train_loss:4.1317 train_time:108350ms step_avg:423.24ms
step:267/1500 train_loss:4.1943 train_time:108773ms step_avg:423.24ms
step:268/1500 train_loss:4.2021 train_time:109195ms step_avg:423.24ms
step:269/1500 train_loss:4.2162 train_time:109618ms step_avg:423.23ms
step:270/1500 train_loss:4.1286 train_time:110040ms step_avg:423.23ms
step:271/1500 train_loss:4.3594 train_time:110462ms step_avg:423.23ms
step:272/1500 train_loss:4.2652 train_time:110885ms step_avg:423.22ms
step:273/1500 train_loss:4.1755 train_time:111307ms step_avg:423.22ms
step:274/1500 train_loss:4.2207 train_time:111730ms step_avg:423.22ms
step:275/1500 train_loss:4.2943 train_time:112153ms step_avg:423.22ms
step:276/1500 train_loss:4.3217 train_time:112575ms step_avg:423.22ms
step:277/1500 train_loss:4.4992 train_time:112998ms step_avg:423.21ms
step:278/1500 train_loss:4.2850 train_time:113422ms step_avg:423.21ms
step:279/1500 train_loss:4.3599 train_time:113845ms step_avg:423.22ms
step:280/1500 train_loss:4.2559 train_time:114267ms step_avg:423.21ms
step:281/1500 train_loss:4.3767 train_time:114689ms step_avg:423.21ms
step:282/1500 train_loss:4.2069 train_time:115111ms step_avg:423.20ms
step:283/1500 train_loss:4.2312 train_time:115535ms step_avg:423.21ms
step:284/1500 train_loss:4.1588 train_time:115961ms step_avg:423.21ms
step:285/1500 train_loss:4.3037 train_time:116387ms step_avg:423.22ms
step:286/1500 train_loss:4.3086 train_time:116809ms step_avg:423.22ms
step:287/1500 train_loss:4.3366 train_time:117234ms step_avg:423.23ms
step:288/1500 train_loss:4.1650 train_time:117657ms step_avg:423.23ms
step:289/1500 train_loss:4.2581 train_time:118080ms step_avg:423.23ms
step:290/1500 train_loss:4.1160 train_time:118502ms step_avg:423.22ms
step:291/1500 train_loss:4.1109 train_time:118923ms step_avg:423.21ms
step:292/1500 train_loss:4.2017 train_time:119345ms step_avg:423.21ms
step:293/1500 train_loss:4.1054 train_time:119767ms step_avg:423.20ms
step:294/1500 train_loss:4.1550 train_time:120191ms step_avg:423.21ms
step:295/1500 train_loss:4.1972 train_time:120614ms step_avg:423.21ms
step:296/1500 train_loss:4.0730 train_time:121038ms step_avg:423.21ms
step:297/1500 train_loss:4.0961 train_time:121461ms step_avg:423.21ms
step:298/1500 train_loss:4.0948 train_time:121885ms step_avg:423.21ms
step:299/1500 train_loss:4.2132 train_time:122308ms step_avg:423.21ms
step:300/1500 train_loss:4.0739 train_time:122730ms step_avg:423.21ms
step:301/1500 train_loss:4.2103 train_time:123153ms step_avg:423.21ms
step:302/1500 train_loss:4.2133 train_time:123576ms step_avg:423.21ms
step:303/1500 train_loss:4.1613 train_time:123998ms step_avg:423.20ms
step:304/1500 train_loss:4.2240 train_time:124421ms step_avg:423.20ms
step:305/1500 train_loss:4.1944 train_time:124845ms step_avg:423.20ms
step:306/1500 train_loss:4.6796 train_time:125269ms step_avg:423.21ms
step:307/1500 train_loss:4.1624 train_time:125694ms step_avg:423.21ms
step:308/1500 train_loss:4.0703 train_time:126120ms step_avg:423.22ms
step:309/1500 train_loss:4.2249 train_time:126544ms step_avg:423.23ms
step:310/1500 train_loss:4.0829 train_time:126966ms step_avg:423.22ms
step:311/1500 train_loss:4.3136 train_time:127389ms step_avg:423.22ms
step:312/1500 train_loss:4.1651 train_time:127815ms step_avg:423.23ms
step:313/1500 train_loss:4.1000 train_time:128238ms step_avg:423.23ms
step:314/1500 train_loss:4.1973 train_time:128661ms step_avg:423.23ms
step:315/1500 train_loss:4.3094 train_time:129087ms step_avg:423.24ms
step:316/1500 train_loss:4.1812 train_time:129509ms step_avg:423.23ms
step:317/1500 train_loss:4.0202 train_time:129931ms step_avg:423.23ms
step:318/1500 train_loss:4.1002 train_time:130356ms step_avg:423.23ms
step:319/1500 train_loss:4.1376 train_time:130777ms step_avg:423.22ms
step:320/1500 train_loss:4.1071 train_time:131199ms step_avg:423.22ms
step:321/1500 train_loss:4.2160 train_time:131621ms step_avg:423.22ms
step:322/1500 train_loss:4.1738 train_time:132045ms step_avg:423.22ms
step:323/1500 train_loss:4.1378 train_time:132468ms step_avg:423.22ms
step:324/1500 train_loss:4.2259 train_time:132892ms step_avg:423.22ms
step:325/1500 train_loss:4.1792 train_time:133315ms step_avg:423.22ms
step:326/1500 train_loss:4.2534 train_time:133739ms step_avg:423.23ms
step:327/1500 train_loss:4.1076 train_time:134163ms step_avg:423.23ms
step:328/1500 train_loss:4.5955 train_time:134587ms step_avg:423.23ms
step:329/1500 train_loss:4.2901 train_time:135012ms step_avg:423.23ms
step:330/1500 train_loss:4.0282 train_time:135437ms step_avg:423.24ms
step:331/1500 train_loss:3.9713 train_time:135861ms step_avg:423.24ms
step:332/1500 train_loss:4.1894 train_time:136284ms step_avg:423.24ms
step:333/1500 train_loss:4.1128 train_time:136707ms step_avg:423.24ms
step:334/1500 train_loss:4.0935 train_time:137130ms step_avg:423.24ms
step:335/1500 train_loss:4.0550 train_time:137553ms step_avg:423.24ms
step:336/1500 train_loss:4.2321 train_time:137976ms step_avg:423.24ms
step:337/1500 train_loss:4.1667 train_time:138398ms step_avg:423.24ms
step:338/1500 train_loss:4.6442 train_time:138820ms step_avg:423.23ms
step:339/1500 train_loss:4.1511 train_time:139241ms step_avg:423.23ms
step:340/1500 train_loss:4.0954 train_time:139665ms step_avg:423.23ms
step:341/1500 train_loss:4.1332 train_time:140090ms step_avg:423.23ms
step:342/1500 train_loss:4.0532 train_time:140513ms step_avg:423.23ms
step:343/1500 train_loss:4.0198 train_time:140935ms step_avg:423.23ms
step:344/1500 train_loss:4.0632 train_time:141359ms step_avg:423.23ms
step:345/1500 train_loss:4.2059 train_time:141782ms step_avg:423.23ms
step:346/1500 train_loss:4.0467 train_time:142206ms step_avg:423.23ms
step:347/1500 train_loss:3.9735 train_time:142630ms step_avg:423.23ms
step:348/1500 train_loss:4.0251 train_time:143051ms step_avg:423.23ms
step:349/1500 train_loss:4.0660 train_time:143473ms step_avg:423.22ms
step:350/1500 train_loss:4.0224 train_time:143896ms step_avg:423.22ms
step:351/1500 train_loss:3.7489 train_time:144318ms step_avg:423.22ms
step:352/1500 train_loss:4.0201 train_time:144742ms step_avg:423.22ms
step:353/1500 train_loss:4.3680 train_time:145165ms step_avg:423.22ms
step:354/1500 train_loss:3.8691 train_time:145589ms step_avg:423.22ms
step:355/1500 train_loss:4.1272 train_time:146012ms step_avg:423.22ms
step:356/1500 train_loss:3.9954 train_time:146435ms step_avg:423.22ms
step:357/1500 train_loss:4.0912 train_time:146860ms step_avg:423.23ms
step:358/1500 train_loss:4.0421 train_time:147284ms step_avg:423.23ms
step:359/1500 train_loss:4.0509 train_time:147708ms step_avg:423.23ms
step:360/1500 train_loss:4.1052 train_time:148131ms step_avg:423.23ms
step:361/1500 train_loss:3.6585 train_time:148554ms step_avg:423.23ms
step:362/1500 train_loss:4.2155 train_time:148976ms step_avg:423.23ms
step:363/1500 train_loss:4.1132 train_time:149398ms step_avg:423.22ms
step:364/1500 train_loss:4.0410 train_time:149822ms step_avg:423.22ms
step:365/1500 train_loss:3.9436 train_time:150243ms step_avg:423.22ms
step:366/1500 train_loss:4.1150 train_time:150666ms step_avg:423.22ms
step:367/1500 train_loss:4.0664 train_time:151090ms step_avg:423.22ms
step:368/1500 train_loss:4.0553 train_time:151513ms step_avg:423.22ms
step:369/1500 train_loss:4.0444 train_time:151938ms step_avg:423.22ms
step:370/1500 train_loss:3.9371 train_time:152360ms step_avg:423.22ms
step:371/1500 train_loss:4.0872 train_time:152782ms step_avg:423.22ms
step:372/1500 train_loss:3.9596 train_time:153205ms step_avg:423.22ms
step:373/1500 train_loss:3.8961 train_time:153626ms step_avg:423.21ms
step:374/1500 train_loss:4.1055 train_time:154048ms step_avg:423.21ms
step:375/1500 train_loss:4.0359 train_time:154470ms step_avg:423.21ms
step:375/1500 val_loss:4.0283 train_time:154483ms step_avg:423.24ms
step:376/1500 train_loss:4.0015 train_time:154890ms step_avg:423.20ms
step:377/1500 train_loss:4.0670 train_time:155313ms step_avg:423.20ms
step:378/1500 train_loss:3.9780 train_time:156685ms step_avg:425.77ms
step:379/1500 train_loss:4.0328 train_time:157114ms step_avg:425.78ms
step:380/1500 train_loss:4.0673 train_time:157712ms step_avg:426.25ms
step:381/1500 train_loss:4.1357 train_time:158140ms step_avg:426.25ms
step:382/1500 train_loss:4.0417 train_time:158565ms step_avg:426.25ms
step:383/1500 train_loss:4.0204 train_time:158988ms step_avg:426.24ms
step:384/1500 train_loss:3.9768 train_time:159412ms step_avg:426.24ms
step:385/1500 train_loss:4.0619 train_time:159837ms step_avg:426.23ms
step:386/1500 train_loss:3.9719 train_time:160260ms step_avg:426.22ms
step:387/1500 train_loss:4.0835 train_time:160686ms step_avg:426.22ms
step:388/1500 train_loss:4.2717 train_time:161109ms step_avg:426.21ms
step:389/1500 train_loss:3.9874 train_time:161533ms step_avg:426.21ms
step:390/1500 train_loss:3.9845 train_time:161958ms step_avg:426.20ms
step:391/1500 train_loss:4.0799 train_time:162380ms step_avg:426.20ms
step:392/1500 train_loss:4.0012 train_time:162809ms step_avg:426.20ms
step:393/1500 train_loss:4.1099 train_time:163234ms step_avg:426.20ms
step:394/1500 train_loss:3.9470 train_time:163657ms step_avg:426.19ms
step:395/1500 train_loss:4.0842 train_time:164081ms step_avg:426.18ms
step:396/1500 train_loss:3.8271 train_time:164504ms step_avg:426.18ms
step:397/1500 train_loss:4.0242 train_time:164929ms step_avg:426.17ms
step:398/1500 train_loss:4.0699 train_time:165354ms step_avg:426.17ms
step:399/1500 train_loss:4.0785 train_time:165777ms step_avg:426.16ms
step:400/1500 train_loss:3.9741 train_time:166200ms step_avg:426.15ms
step:401/1500 train_loss:4.0427 train_time:166624ms step_avg:426.15ms
step:402/1500 train_loss:4.1025 train_time:167049ms step_avg:426.14ms
step:403/1500 train_loss:4.0343 train_time:167471ms step_avg:426.14ms
step:404/1500 train_loss:4.1436 train_time:167894ms step_avg:426.13ms
step:405/1500 train_loss:3.8896 train_time:168317ms step_avg:426.12ms
step:406/1500 train_loss:3.9766 train_time:168741ms step_avg:426.11ms
step:407/1500 train_loss:4.2757 train_time:169165ms step_avg:426.11ms
step:408/1500 train_loss:3.9873 train_time:169587ms step_avg:426.10ms
step:409/1500 train_loss:4.0059 train_time:170008ms step_avg:426.08ms
step:410/1500 train_loss:4.0526 train_time:170432ms step_avg:426.08ms
step:411/1500 train_loss:3.9343 train_time:170854ms step_avg:426.07ms
step:412/1500 train_loss:3.9549 train_time:171279ms step_avg:426.07ms
step:413/1500 train_loss:4.3755 train_time:171701ms step_avg:426.06ms
step:414/1500 train_loss:3.8248 train_time:172125ms step_avg:426.05ms
step:415/1500 train_loss:4.1995 train_time:172550ms step_avg:426.05ms
step:416/1500 train_loss:3.9470 train_time:172973ms step_avg:426.04ms
step:417/1500 train_loss:3.9555 train_time:173397ms step_avg:426.04ms
step:418/1500 train_loss:4.1450 train_time:173820ms step_avg:426.03ms
step:419/1500 train_loss:3.8798 train_time:174241ms step_avg:426.02ms
step:420/1500 train_loss:3.9886 train_time:174665ms step_avg:426.01ms
step:421/1500 train_loss:3.9163 train_time:175086ms step_avg:426.00ms
step:422/1500 train_loss:3.8279 train_time:175509ms step_avg:425.99ms
step:423/1500 train_loss:3.9644 train_time:175931ms step_avg:425.98ms
step:424/1500 train_loss:4.0546 train_time:176353ms step_avg:425.97ms
step:425/1500 train_loss:3.8105 train_time:176777ms step_avg:425.97ms
step:426/1500 train_loss:3.9910 train_time:177199ms step_avg:425.96ms
step:427/1500 train_loss:3.8697 train_time:177621ms step_avg:425.95ms
step:428/1500 train_loss:4.0877 train_time:178044ms step_avg:425.94ms
step:429/1500 train_loss:4.0014 train_time:178471ms step_avg:425.95ms
step:430/1500 train_loss:3.9403 train_time:178894ms step_avg:425.94ms
step:431/1500 train_loss:3.9057 train_time:179317ms step_avg:425.93ms
step:432/1500 train_loss:3.8083 train_time:179738ms step_avg:425.92ms
step:433/1500 train_loss:3.9472 train_time:180160ms step_avg:425.91ms
step:434/1500 train_loss:4.0108 train_time:180583ms step_avg:425.90ms
step:435/1500 train_loss:3.9465 train_time:181005ms step_avg:425.89ms
step:436/1500 train_loss:3.9983 train_time:181431ms step_avg:425.89ms
step:437/1500 train_loss:4.0112 train_time:181854ms step_avg:425.89ms
step:438/1500 train_loss:3.8958 train_time:182276ms step_avg:425.88ms
step:439/1500 train_loss:3.9031 train_time:182699ms step_avg:425.87ms
step:440/1500 train_loss:3.8863 train_time:183123ms step_avg:425.87ms
step:441/1500 train_loss:4.0594 train_time:183547ms step_avg:425.86ms
step:442/1500 train_loss:3.9499 train_time:183969ms step_avg:425.85ms
step:443/1500 train_loss:3.9356 train_time:184393ms step_avg:425.85ms
step:444/1500 train_loss:3.8214 train_time:184816ms step_avg:425.84ms
step:445/1500 train_loss:4.0979 train_time:185239ms step_avg:425.84ms
step:446/1500 train_loss:4.0254 train_time:185665ms step_avg:425.84ms
step:447/1500 train_loss:4.0106 train_time:186089ms step_avg:425.83ms
step:448/1500 train_loss:3.9315 train_time:186512ms step_avg:425.83ms
step:449/1500 train_loss:4.0311 train_time:186935ms step_avg:425.82ms
step:450/1500 train_loss:3.8558 train_time:187359ms step_avg:425.82ms
step:451/1500 train_loss:3.8910 train_time:187783ms step_avg:425.81ms
step:452/1500 train_loss:3.7623 train_time:188207ms step_avg:425.81ms
step:453/1500 train_loss:3.8859 train_time:188629ms step_avg:425.80ms
step:454/1500 train_loss:3.8533 train_time:189054ms step_avg:425.80ms
step:455/1500 train_loss:3.8110 train_time:189476ms step_avg:425.79ms
step:456/1500 train_loss:4.0283 train_time:189898ms step_avg:425.78ms
step:457/1500 train_loss:3.9080 train_time:190322ms step_avg:425.78ms
step:458/1500 train_loss:3.9760 train_time:190743ms step_avg:425.77ms
step:459/1500 train_loss:4.0027 train_time:191166ms step_avg:425.76ms
step:460/1500 train_loss:3.8143 train_time:191590ms step_avg:425.76ms
step:461/1500 train_loss:3.9782 train_time:192012ms step_avg:425.75ms
step:462/1500 train_loss:3.8719 train_time:192434ms step_avg:425.74ms
step:463/1500 train_loss:3.9003 train_time:192859ms step_avg:425.74ms
step:464/1500 train_loss:3.9536 train_time:193282ms step_avg:425.73ms
step:465/1500 train_loss:3.8956 train_time:193705ms step_avg:425.73ms
step:466/1500 train_loss:3.9057 train_time:194128ms step_avg:425.72ms
step:467/1500 train_loss:3.9863 train_time:194551ms step_avg:425.71ms
step:468/1500 train_loss:4.0090 train_time:194973ms step_avg:425.70ms
step:469/1500 train_loss:3.9778 train_time:195394ms step_avg:425.69ms
step:470/1500 train_loss:3.8688 train_time:195819ms step_avg:425.69ms
step:471/1500 train_loss:3.9506 train_time:196241ms step_avg:425.69ms
step:472/1500 train_loss:4.0036 train_time:196663ms step_avg:425.68ms
step:473/1500 train_loss:3.9479 train_time:197085ms step_avg:425.67ms
step:474/1500 train_loss:3.9031 train_time:197508ms step_avg:425.66ms
step:475/1500 train_loss:3.7593 train_time:197930ms step_avg:425.66ms
step:476/1500 train_loss:4.2117 train_time:198352ms step_avg:425.65ms
step:477/1500 train_loss:3.9460 train_time:198775ms step_avg:425.64ms
step:478/1500 train_loss:3.7576 train_time:199196ms step_avg:425.63ms
step:479/1500 train_loss:3.9898 train_time:199619ms step_avg:425.63ms
step:480/1500 train_loss:3.9392 train_time:200041ms step_avg:425.62ms
step:481/1500 train_loss:4.0872 train_time:200465ms step_avg:425.62ms
step:482/1500 train_loss:3.9003 train_time:200887ms step_avg:425.61ms
step:483/1500 train_loss:3.7037 train_time:201310ms step_avg:425.60ms
step:484/1500 train_loss:3.9896 train_time:201732ms step_avg:425.60ms
step:485/1500 train_loss:3.8403 train_time:202154ms step_avg:425.59ms
step:486/1500 train_loss:3.8469 train_time:202575ms step_avg:425.58ms
step:487/1500 train_loss:3.7769 train_time:202998ms step_avg:425.57ms
step:488/1500 train_loss:3.8516 train_time:203420ms step_avg:425.57ms
step:489/1500 train_loss:4.0457 train_time:203842ms step_avg:425.56ms
step:490/1500 train_loss:3.8924 train_time:204263ms step_avg:425.55ms
step:491/1500 train_loss:3.7810 train_time:204686ms step_avg:425.54ms
step:492/1500 train_loss:3.7902 train_time:205108ms step_avg:425.54ms
step:493/1500 train_loss:3.9112 train_time:205531ms step_avg:425.53ms
step:494/1500 train_loss:3.7512 train_time:205953ms step_avg:425.52ms
step:495/1500 train_loss:3.8905 train_time:206374ms step_avg:425.51ms
step:496/1500 train_loss:3.8290 train_time:206797ms step_avg:425.51ms
step:497/1500 train_loss:3.7065 train_time:207218ms step_avg:425.50ms
step:498/1500 train_loss:3.9065 train_time:207640ms step_avg:425.49ms
step:499/1500 train_loss:3.9790 train_time:208062ms step_avg:425.48ms
step:500/1500 train_loss:4.0042 train_time:208485ms step_avg:425.48ms
step:500/1500 val_loss:3.8834 train_time:208499ms step_avg:425.51ms
step:501/1500 train_loss:3.9186 train_time:208907ms step_avg:425.47ms
step:502/1500 train_loss:3.9760 train_time:209329ms step_avg:425.47ms
step:503/1500 train_loss:3.9178 train_time:209752ms step_avg:425.46ms
step:504/1500 train_loss:3.9579 train_time:210176ms step_avg:425.46ms
step:505/1500 train_loss:3.9027 train_time:210599ms step_avg:425.45ms
step:506/1500 train_loss:3.9931 train_time:211021ms step_avg:425.44ms
step:507/1500 train_loss:3.8062 train_time:211443ms step_avg:425.44ms
step:508/1500 train_loss:3.9335 train_time:211865ms step_avg:425.43ms
step:509/1500 train_loss:4.0056 train_time:212287ms step_avg:425.42ms
step:510/1500 train_loss:3.9474 train_time:212710ms step_avg:425.42ms
step:511/1500 train_loss:3.7509 train_time:213132ms step_avg:425.41ms
step:512/1500 train_loss:3.9534 train_time:213556ms step_avg:425.41ms
step:513/1500 train_loss:3.8949 train_time:213977ms step_avg:425.40ms
step:514/1500 train_loss:3.8558 train_time:214399ms step_avg:425.40ms
step:515/1500 train_loss:3.9357 train_time:214822ms step_avg:425.39ms
step:516/1500 train_loss:3.9158 train_time:215248ms step_avg:425.39ms
step:517/1500 train_loss:4.2578 train_time:215670ms step_avg:425.38ms
step:518/1500 train_loss:3.8565 train_time:216092ms step_avg:425.38ms
step:519/1500 train_loss:3.9628 train_time:216515ms step_avg:425.37ms
step:520/1500 train_loss:3.8607 train_time:216938ms step_avg:425.37ms
step:521/1500 train_loss:3.8628 train_time:217360ms step_avg:425.36ms
step:522/1500 train_loss:3.8119 train_time:217782ms step_avg:425.36ms
step:523/1500 train_loss:3.8296 train_time:218206ms step_avg:425.35ms
step:524/1500 train_loss:4.4532 train_time:218627ms step_avg:425.34ms
step:525/1500 train_loss:3.9173 train_time:219049ms step_avg:425.34ms
step:526/1500 train_loss:3.8512 train_time:219473ms step_avg:425.33ms
step:527/1500 train_loss:3.8675 train_time:219894ms step_avg:425.33ms
step:528/1500 train_loss:3.8228 train_time:220316ms step_avg:425.32ms
step:529/1500 train_loss:3.7926 train_time:220740ms step_avg:425.32ms
step:530/1500 train_loss:4.0160 train_time:221162ms step_avg:425.31ms
step:531/1500 train_loss:3.8126 train_time:221584ms step_avg:425.31ms
step:532/1500 train_loss:4.0874 train_time:222005ms step_avg:425.30ms
step:533/1500 train_loss:3.9067 train_time:222427ms step_avg:425.29ms
step:534/1500 train_loss:3.8229 train_time:222851ms step_avg:425.29ms
step:535/1500 train_loss:3.8488 train_time:223273ms step_avg:425.28ms
step:536/1500 train_loss:3.7857 train_time:223695ms step_avg:425.28ms
step:537/1500 train_loss:3.9173 train_time:224117ms step_avg:425.27ms
step:538/1500 train_loss:3.9000 train_time:224542ms step_avg:425.27ms
step:539/1500 train_loss:3.8005 train_time:224966ms step_avg:425.27ms
step:540/1500 train_loss:4.2926 train_time:225388ms step_avg:425.26ms
step:541/1500 train_loss:3.8368 train_time:225811ms step_avg:425.26ms
step:542/1500 train_loss:3.9519 train_time:226235ms step_avg:425.25ms
step:543/1500 train_loss:3.7768 train_time:226658ms step_avg:425.25ms
step:544/1500 train_loss:3.7497 train_time:227081ms step_avg:425.24ms
step:545/1500 train_loss:3.8351 train_time:227502ms step_avg:425.24ms
step:546/1500 train_loss:3.7599 train_time:227923ms step_avg:425.23ms
step:547/1500 train_loss:3.8076 train_time:228344ms step_avg:425.22ms
step:548/1500 train_loss:3.8248 train_time:228767ms step_avg:425.22ms
step:549/1500 train_loss:3.7909 train_time:229190ms step_avg:425.21ms
step:550/1500 train_loss:3.8955 train_time:229614ms step_avg:425.21ms
step:551/1500 train_loss:3.7784 train_time:230037ms step_avg:425.21ms
step:552/1500 train_loss:3.7904 train_time:230460ms step_avg:425.20ms
step:553/1500 train_loss:4.1180 train_time:230882ms step_avg:425.20ms
step:554/1500 train_loss:3.9237 train_time:231305ms step_avg:425.19ms
step:555/1500 train_loss:3.8766 train_time:231728ms step_avg:425.19ms
step:556/1500 train_loss:3.8162 train_time:232151ms step_avg:425.19ms
step:557/1500 train_loss:3.8580 train_time:232574ms step_avg:425.18ms
step:558/1500 train_loss:3.5185 train_time:232995ms step_avg:425.17ms
step:559/1500 train_loss:3.7783 train_time:233417ms step_avg:425.17ms
step:560/1500 train_loss:3.8201 train_time:233841ms step_avg:425.16ms
step:561/1500 train_loss:3.8667 train_time:234264ms step_avg:425.16ms
step:562/1500 train_loss:3.7761 train_time:234685ms step_avg:425.15ms
step:563/1500 train_loss:3.7166 train_time:235109ms step_avg:425.15ms
step:564/1500 train_loss:3.9295 train_time:235530ms step_avg:425.14ms
step:565/1500 train_loss:3.7335 train_time:235950ms step_avg:425.14ms
step:566/1500 train_loss:3.8559 train_time:236374ms step_avg:425.13ms
step:567/1500 train_loss:3.8027 train_time:237604ms step_avg:426.58ms
step:568/1500 train_loss:3.7564 train_time:238022ms step_avg:426.56ms
step:569/1500 train_loss:3.8477 train_time:238436ms step_avg:426.54ms
step:570/1500 train_loss:3.8217 train_time:238978ms step_avg:426.75ms
step:571/1500 train_loss:3.8515 train_time:239393ms step_avg:426.73ms
step:572/1500 train_loss:3.9341 train_time:239808ms step_avg:426.70ms
step:573/1500 train_loss:3.8912 train_time:240225ms step_avg:426.69ms
step:574/1500 train_loss:3.8917 train_time:240641ms step_avg:426.67ms
step:575/1500 train_loss:3.9354 train_time:241056ms step_avg:426.65ms
step:576/1500 train_loss:3.8965 train_time:241471ms step_avg:426.63ms
step:577/1500 train_loss:3.9211 train_time:241890ms step_avg:426.61ms
step:578/1500 train_loss:3.8482 train_time:242306ms step_avg:426.59ms
step:579/1500 train_loss:3.8385 train_time:242720ms step_avg:426.57ms
step:580/1500 train_loss:3.8237 train_time:243137ms step_avg:426.56ms
step:581/1500 train_loss:3.7665 train_time:243553ms step_avg:426.54ms
step:582/1500 train_loss:3.7914 train_time:243969ms step_avg:426.52ms
step:583/1500 train_loss:4.0177 train_time:244384ms step_avg:426.50ms
step:584/1500 train_loss:3.7916 train_time:244801ms step_avg:426.48ms
step:585/1500 train_loss:3.7591 train_time:245217ms step_avg:426.46ms
step:586/1500 train_loss:3.9411 train_time:245633ms step_avg:426.45ms
step:587/1500 train_loss:3.6963 train_time:246051ms step_avg:426.43ms
step:588/1500 train_loss:3.8325 train_time:246468ms step_avg:426.42ms
step:589/1500 train_loss:3.8171 train_time:246884ms step_avg:426.40ms
step:590/1500 train_loss:4.1642 train_time:247300ms step_avg:426.38ms
step:591/1500 train_loss:3.9446 train_time:247716ms step_avg:426.36ms
step:592/1500 train_loss:3.6853 train_time:248133ms step_avg:426.34ms
step:593/1500 train_loss:3.6936 train_time:248548ms step_avg:426.33ms
step:594/1500 train_loss:3.6864 train_time:248964ms step_avg:426.31ms
step:595/1500 train_loss:3.7257 train_time:249380ms step_avg:426.29ms
step:596/1500 train_loss:4.0921 train_time:249796ms step_avg:426.27ms
step:597/1500 train_loss:3.8123 train_time:250214ms step_avg:426.26ms
step:598/1500 train_loss:3.7494 train_time:250630ms step_avg:426.24ms
step:599/1500 train_loss:3.8266 train_time:251046ms step_avg:426.22ms
step:600/1500 train_loss:3.6404 train_time:251462ms step_avg:426.21ms
step:601/1500 train_loss:3.7599 train_time:251878ms step_avg:426.19ms
step:602/1500 train_loss:3.7972 train_time:252295ms step_avg:426.17ms
step:603/1500 train_loss:3.8185 train_time:252709ms step_avg:426.15ms
step:604/1500 train_loss:3.9420 train_time:253128ms step_avg:426.14ms
step:605/1500 train_loss:3.7960 train_time:253545ms step_avg:426.13ms
step:606/1500 train_loss:3.7779 train_time:253962ms step_avg:426.11ms
step:607/1500 train_loss:3.7319 train_time:254378ms step_avg:426.09ms
step:608/1500 train_loss:3.9790 train_time:254794ms step_avg:426.08ms
step:609/1500 train_loss:3.8078 train_time:255210ms step_avg:426.06ms
step:610/1500 train_loss:3.7822 train_time:255625ms step_avg:426.04ms
step:611/1500 train_loss:3.8794 train_time:256042ms step_avg:426.03ms
step:612/1500 train_loss:3.7832 train_time:256458ms step_avg:426.01ms
step:613/1500 train_loss:3.7699 train_time:256874ms step_avg:425.99ms
step:614/1500 train_loss:3.9247 train_time:257295ms step_avg:425.98ms
step:615/1500 train_loss:3.8793 train_time:257710ms step_avg:425.97ms
step:616/1500 train_loss:3.8561 train_time:258126ms step_avg:425.95ms
step:617/1500 train_loss:3.7779 train_time:258541ms step_avg:425.93ms
step:618/1500 train_loss:3.7369 train_time:258956ms step_avg:425.91ms
step:619/1500 train_loss:3.8413 train_time:259372ms step_avg:425.90ms
step:620/1500 train_loss:3.7404 train_time:259792ms step_avg:425.89ms
step:621/1500 train_loss:3.7499 train_time:260209ms step_avg:425.87ms
step:622/1500 train_loss:4.0653 train_time:260625ms step_avg:425.86ms
step:623/1500 train_loss:3.7480 train_time:261041ms step_avg:425.84ms
step:624/1500 train_loss:3.7769 train_time:261456ms step_avg:425.82ms
step:625/1500 train_loss:3.8681 train_time:261873ms step_avg:425.81ms
step:625/1500 val_loss:3.7884 train_time:261891ms step_avg:425.84ms
step:626/1500 train_loss:3.8831 train_time:262297ms step_avg:425.81ms
step:627/1500 train_loss:3.9063 train_time:262712ms step_avg:425.79ms
step:628/1500 train_loss:3.8895 train_time:263126ms step_avg:425.77ms
step:629/1500 train_loss:3.9336 train_time:263542ms step_avg:425.75ms
step:630/1500 train_loss:3.7567 train_time:263961ms step_avg:425.74ms
step:631/1500 train_loss:3.8839 train_time:264377ms step_avg:425.73ms
step:632/1500 train_loss:3.9124 train_time:264793ms step_avg:425.71ms
step:633/1500 train_loss:3.8152 train_time:265211ms step_avg:425.70ms
step:634/1500 train_loss:3.7494 train_time:265628ms step_avg:425.69ms
step:635/1500 train_loss:3.8427 train_time:266045ms step_avg:425.67ms
step:636/1500 train_loss:4.0996 train_time:266464ms step_avg:425.66ms
step:637/1500 train_loss:3.6944 train_time:266879ms step_avg:425.64ms
step:638/1500 train_loss:3.5163 train_time:267295ms step_avg:425.63ms
step:639/1500 train_loss:3.7445 train_time:267710ms step_avg:425.61ms
step:640/1500 train_loss:3.7789 train_time:268125ms step_avg:425.60ms
step:641/1500 train_loss:3.7284 train_time:268540ms step_avg:425.58ms
step:642/1500 train_loss:3.7381 train_time:268961ms step_avg:425.57ms
step:643/1500 train_loss:3.7804 train_time:269378ms step_avg:425.56ms
step:644/1500 train_loss:3.7904 train_time:269794ms step_avg:425.54ms
step:645/1500 train_loss:3.7208 train_time:270210ms step_avg:425.53ms
step:646/1500 train_loss:3.9377 train_time:270627ms step_avg:425.51ms
step:647/1500 train_loss:3.8314 train_time:271042ms step_avg:425.50ms
step:648/1500 train_loss:3.8313 train_time:271463ms step_avg:425.49ms
step:649/1500 train_loss:3.8574 train_time:271878ms step_avg:425.47ms
step:650/1500 train_loss:3.9212 train_time:272295ms step_avg:425.46ms
step:651/1500 train_loss:3.7866 train_time:272711ms step_avg:425.45ms
step:652/1500 train_loss:3.9228 train_time:273126ms step_avg:425.43ms
step:653/1500 train_loss:3.7433 train_time:273542ms step_avg:425.42ms
step:654/1500 train_loss:3.8214 train_time:273963ms step_avg:425.41ms
step:655/1500 train_loss:3.5845 train_time:274380ms step_avg:425.39ms
step:656/1500 train_loss:3.7322 train_time:274797ms step_avg:425.38ms
step:657/1500 train_loss:3.7388 train_time:275214ms step_avg:425.37ms
step:658/1500 train_loss:3.6704 train_time:275631ms step_avg:425.36ms
step:659/1500 train_loss:3.8485 train_time:276048ms step_avg:425.34ms
step:660/1500 train_loss:3.7456 train_time:276464ms step_avg:425.33ms
step:661/1500 train_loss:3.8425 train_time:276881ms step_avg:425.32ms
step:662/1500 train_loss:3.9113 train_time:277296ms step_avg:425.30ms
step:663/1500 train_loss:3.8277 train_time:277716ms step_avg:425.29ms
step:664/1500 train_loss:3.7020 train_time:278132ms step_avg:425.28ms
step:665/1500 train_loss:3.7839 train_time:278548ms step_avg:425.26ms
step:666/1500 train_loss:3.6567 train_time:278963ms step_avg:425.25ms
step:667/1500 train_loss:3.9430 train_time:279381ms step_avg:425.24ms
step:668/1500 train_loss:3.7755 train_time:279798ms step_avg:425.23ms
step:669/1500 train_loss:3.7900 train_time:280214ms step_avg:425.21ms
step:670/1500 train_loss:3.6416 train_time:280630ms step_avg:425.20ms
step:671/1500 train_loss:3.7566 train_time:281048ms step_avg:425.19ms
step:672/1500 train_loss:3.7170 train_time:281475ms step_avg:425.19ms
step:673/1500 train_loss:3.7328 train_time:281881ms step_avg:425.16ms
step:674/1500 train_loss:4.0143 train_time:282298ms step_avg:425.15ms
step:675/1500 train_loss:3.8010 train_time:282713ms step_avg:425.13ms
step:676/1500 train_loss:3.8706 train_time:283130ms step_avg:425.12ms
step:677/1500 train_loss:3.6504 train_time:283544ms step_avg:425.10ms
step:678/1500 train_loss:3.7534 train_time:283961ms step_avg:425.09ms
step:679/1500 train_loss:3.6984 train_time:284377ms step_avg:425.08ms
step:680/1500 train_loss:3.8375 train_time:284794ms step_avg:425.07ms
step:681/1500 train_loss:3.7434 train_time:285210ms step_avg:425.05ms
step:682/1500 train_loss:3.7706 train_time:285627ms step_avg:425.04ms
step:683/1500 train_loss:3.8509 train_time:286043ms step_avg:425.03ms
step:684/1500 train_loss:3.8918 train_time:286463ms step_avg:425.02ms
step:685/1500 train_loss:3.7860 train_time:286878ms step_avg:425.01ms
step:686/1500 train_loss:3.8608 train_time:287294ms step_avg:424.99ms
step:687/1500 train_loss:3.7907 train_time:287711ms step_avg:424.98ms
step:688/1500 train_loss:3.8312 train_time:288126ms step_avg:424.96ms
step:689/1500 train_loss:3.4580 train_time:288542ms step_avg:424.95ms
step:690/1500 train_loss:3.5675 train_time:288962ms step_avg:424.94ms
step:691/1500 train_loss:3.7095 train_time:289377ms step_avg:424.93ms
step:692/1500 train_loss:3.5869 train_time:289793ms step_avg:424.92ms
step:693/1500 train_loss:3.7969 train_time:290209ms step_avg:424.90ms
step:694/1500 train_loss:3.8242 train_time:290626ms step_avg:424.89ms
step:695/1500 train_loss:3.7046 train_time:291041ms step_avg:424.88ms
step:696/1500 train_loss:3.6914 train_time:291461ms step_avg:424.87ms
step:697/1500 train_loss:4.0108 train_time:291877ms step_avg:424.86ms
step:698/1500 train_loss:3.7553 train_time:292292ms step_avg:424.84ms
step:699/1500 train_loss:3.7981 train_time:292707ms step_avg:424.83ms
step:700/1500 train_loss:3.9503 train_time:293123ms step_avg:424.82ms
step:701/1500 train_loss:3.7334 train_time:293540ms step_avg:424.80ms
step:702/1500 train_loss:3.6913 train_time:293960ms step_avg:424.80ms
step:703/1500 train_loss:3.6768 train_time:294376ms step_avg:424.79ms
step:704/1500 train_loss:3.6401 train_time:294793ms step_avg:424.77ms
step:705/1500 train_loss:3.7198 train_time:295209ms step_avg:424.76ms
step:706/1500 train_loss:3.7186 train_time:295625ms step_avg:424.75ms
step:707/1500 train_loss:3.7309 train_time:296042ms step_avg:424.74ms
step:708/1500 train_loss:3.8013 train_time:296462ms step_avg:424.73ms
step:709/1500 train_loss:3.7453 train_time:297296ms step_avg:425.32ms
step:710/1500 train_loss:3.7282 train_time:297711ms step_avg:425.30ms
step:711/1500 train_loss:3.6972 train_time:298127ms step_avg:425.29ms
step:712/1500 train_loss:3.7440 train_time:298544ms step_avg:425.28ms
step:713/1500 train_loss:3.8039 train_time:298964ms step_avg:425.27ms
step:714/1500 train_loss:3.8101 train_time:299379ms step_avg:425.25ms
step:715/1500 train_loss:3.7186 train_time:299795ms step_avg:425.24ms
step:716/1500 train_loss:3.7215 train_time:300212ms step_avg:425.23ms
step:717/1500 train_loss:3.7388 train_time:300628ms step_avg:425.22ms
step:718/1500 train_loss:3.8840 train_time:301044ms step_avg:425.20ms
step:719/1500 train_loss:3.7494 train_time:301464ms step_avg:425.20ms
step:720/1500 train_loss:3.8233 train_time:301880ms step_avg:425.18ms
step:721/1500 train_loss:3.9939 train_time:302294ms step_avg:425.17ms
step:722/1500 train_loss:3.6143 train_time:302711ms step_avg:425.16ms
step:723/1500 train_loss:3.8742 train_time:303127ms step_avg:425.14ms
step:724/1500 train_loss:3.9361 train_time:303542ms step_avg:425.13ms
step:725/1500 train_loss:3.7173 train_time:303961ms step_avg:425.12ms
step:726/1500 train_loss:3.7977 train_time:304378ms step_avg:425.11ms
step:727/1500 train_loss:3.6949 train_time:304795ms step_avg:425.10ms
step:728/1500 train_loss:3.7165 train_time:305210ms step_avg:425.08ms
step:729/1500 train_loss:3.8904 train_time:305627ms step_avg:425.07ms
step:730/1500 train_loss:3.8323 train_time:306044ms step_avg:425.06ms
step:731/1500 train_loss:3.8339 train_time:306464ms step_avg:425.05ms
step:732/1500 train_loss:3.7154 train_time:306879ms step_avg:425.04ms
step:733/1500 train_loss:3.7400 train_time:307295ms step_avg:425.03ms
step:734/1500 train_loss:3.9770 train_time:307712ms step_avg:425.02ms
step:735/1500 train_loss:3.7101 train_time:308127ms step_avg:425.00ms
step:736/1500 train_loss:3.7770 train_time:308545ms step_avg:424.99ms
step:737/1500 train_loss:3.8943 train_time:308963ms step_avg:424.98ms
step:738/1500 train_loss:3.8098 train_time:309377ms step_avg:424.97ms
step:739/1500 train_loss:3.7570 train_time:309794ms step_avg:424.96ms
step:740/1500 train_loss:3.6520 train_time:310209ms step_avg:424.94ms
step:741/1500 train_loss:4.2954 train_time:310625ms step_avg:424.93ms
step:742/1500 train_loss:3.6501 train_time:311040ms step_avg:424.92ms
step:743/1500 train_loss:3.7256 train_time:311460ms step_avg:424.91ms
step:744/1500 train_loss:3.7332 train_time:311875ms step_avg:424.90ms
step:745/1500 train_loss:3.7968 train_time:312290ms step_avg:424.88ms
step:746/1500 train_loss:3.7625 train_time:312708ms step_avg:424.87ms
step:747/1500 train_loss:3.7534 train_time:313124ms step_avg:424.86ms
step:748/1500 train_loss:3.7844 train_time:313540ms step_avg:424.85ms
step:749/1500 train_loss:3.7105 train_time:313960ms step_avg:424.84ms
step:750/1500 train_loss:3.7101 train_time:314376ms step_avg:424.83ms
step:750/1500 val_loss:3.7222 train_time:314391ms step_avg:424.85ms
step:751/1500 train_loss:3.7429 train_time:314800ms step_avg:424.83ms
step:752/1500 train_loss:3.7114 train_time:315215ms step_avg:424.82ms
step:753/1500 train_loss:3.7522 train_time:315631ms step_avg:424.81ms
step:754/1500 train_loss:3.7694 train_time:316047ms step_avg:424.79ms
step:755/1500 train_loss:3.7415 train_time:316462ms step_avg:424.78ms
step:756/1500 train_loss:3.8171 train_time:317693ms step_avg:425.86ms
step:757/1500 train_loss:3.6418 train_time:318110ms step_avg:425.85ms
step:758/1500 train_loss:3.8860 train_time:318529ms step_avg:425.84ms
step:759/1500 train_loss:3.7964 train_time:318946ms step_avg:425.83ms
step:760/1500 train_loss:3.7302 train_time:319488ms step_avg:425.98ms
step:761/1500 train_loss:3.8453 train_time:319901ms step_avg:425.97ms
step:762/1500 train_loss:3.5547 train_time:320318ms step_avg:425.95ms
step:763/1500 train_loss:3.7031 train_time:320735ms step_avg:425.94ms
step:764/1500 train_loss:3.8186 train_time:321151ms step_avg:425.93ms
step:765/1500 train_loss:3.4699 train_time:321566ms step_avg:425.92ms
step:766/1500 train_loss:3.8923 train_time:321983ms step_avg:425.90ms
step:767/1500 train_loss:3.7396 train_time:322398ms step_avg:425.89ms
step:768/1500 train_loss:3.7057 train_time:322814ms step_avg:425.88ms
step:769/1500 train_loss:3.7258 train_time:323232ms step_avg:425.87ms
step:770/1500 train_loss:3.7441 train_time:323647ms step_avg:425.85ms
step:771/1500 train_loss:3.8044 train_time:324064ms step_avg:425.84ms
step:772/1500 train_loss:4.0254 train_time:324480ms step_avg:425.83ms
step:773/1500 train_loss:3.6109 train_time:324896ms step_avg:425.81ms
step:774/1500 train_loss:3.8015 train_time:325312ms step_avg:425.80ms
step:775/1500 train_loss:3.7869 train_time:325732ms step_avg:425.79ms
step:776/1500 train_loss:3.7552 train_time:326148ms step_avg:425.78ms
step:777/1500 train_loss:3.5659 train_time:326564ms step_avg:425.77ms
step:778/1500 train_loss:3.5596 train_time:326981ms step_avg:425.76ms
step:779/1500 train_loss:3.6299 train_time:327399ms step_avg:425.75ms
step:780/1500 train_loss:3.7183 train_time:327814ms step_avg:425.73ms
step:781/1500 train_loss:3.7448 train_time:328232ms step_avg:425.72ms
step:782/1500 train_loss:3.8092 train_time:328648ms step_avg:425.71ms
step:783/1500 train_loss:3.7204 train_time:329063ms step_avg:425.70ms
step:784/1500 train_loss:3.7195 train_time:329478ms step_avg:425.68ms
step:785/1500 train_loss:3.7272 train_time:329894ms step_avg:425.67ms
step:786/1500 train_loss:3.7007 train_time:330309ms step_avg:425.66ms
step:787/1500 train_loss:3.6049 train_time:330728ms step_avg:425.65ms
step:788/1500 train_loss:3.8547 train_time:331143ms step_avg:425.63ms
step:789/1500 train_loss:3.6509 train_time:331560ms step_avg:425.62ms
step:790/1500 train_loss:3.7130 train_time:331976ms step_avg:425.61ms
step:791/1500 train_loss:3.7748 train_time:332394ms step_avg:425.60ms
step:792/1500 train_loss:3.9105 train_time:332862ms step_avg:425.65ms
step:793/1500 train_loss:3.9144 train_time:333278ms step_avg:425.64ms
step:794/1500 train_loss:3.6139 train_time:333694ms step_avg:425.63ms
step:795/1500 train_loss:3.7554 train_time:334110ms step_avg:425.62ms
step:796/1500 train_loss:3.8081 train_time:334530ms step_avg:425.61ms
step:797/1500 train_loss:3.9071 train_time:334947ms step_avg:425.60ms
step:798/1500 train_loss:3.6655 train_time:335363ms step_avg:425.59ms
step:799/1500 train_loss:3.8128 train_time:335779ms step_avg:425.58ms
step:800/1500 train_loss:3.7025 train_time:336196ms step_avg:425.56ms
step:801/1500 train_loss:3.6887 train_time:336611ms step_avg:425.55ms
step:802/1500 train_loss:3.7801 train_time:337031ms step_avg:425.54ms
step:803/1500 train_loss:3.6407 train_time:337446ms step_avg:425.53ms
step:804/1500 train_loss:3.6515 train_time:337861ms step_avg:425.52ms
step:805/1500 train_loss:3.7769 train_time:338276ms step_avg:425.50ms
step:806/1500 train_loss:3.6788 train_time:338693ms step_avg:425.49ms
step:807/1500 train_loss:3.6935 train_time:339109ms step_avg:425.48ms
step:808/1500 train_loss:3.7912 train_time:339528ms step_avg:425.47ms
step:809/1500 train_loss:3.7042 train_time:339944ms step_avg:425.46ms
step:810/1500 train_loss:3.6324 train_time:340359ms step_avg:425.45ms
step:811/1500 train_loss:3.7165 train_time:340776ms step_avg:425.44ms
step:812/1500 train_loss:3.7437 train_time:341191ms step_avg:425.43ms
step:813/1500 train_loss:3.7454 train_time:341608ms step_avg:425.41ms
step:814/1500 train_loss:3.7743 train_time:342029ms step_avg:425.41ms
step:815/1500 train_loss:3.7219 train_time:342445ms step_avg:425.40ms
step:816/1500 train_loss:3.7054 train_time:342861ms step_avg:425.39ms
step:817/1500 train_loss:3.8092 train_time:343278ms step_avg:425.37ms
step:818/1500 train_loss:3.9085 train_time:343694ms step_avg:425.36ms
step:819/1500 train_loss:3.6734 train_time:344111ms step_avg:425.35ms
step:820/1500 train_loss:3.8718 train_time:344531ms step_avg:425.35ms
step:821/1500 train_loss:3.6511 train_time:344947ms step_avg:425.34ms
step:822/1500 train_loss:3.6954 train_time:345363ms step_avg:425.32ms
step:823/1500 train_loss:3.8204 train_time:345778ms step_avg:425.31ms
step:824/1500 train_loss:3.7326 train_time:346194ms step_avg:425.30ms
step:825/1500 train_loss:3.6639 train_time:346610ms step_avg:425.29ms
step:826/1500 train_loss:3.7610 train_time:347032ms step_avg:425.28ms
step:827/1500 train_loss:3.6490 train_time:347447ms step_avg:425.27ms
step:828/1500 train_loss:3.8776 train_time:347862ms step_avg:425.26ms
step:829/1500 train_loss:3.7671 train_time:348279ms step_avg:425.25ms
step:830/1500 train_loss:3.8216 train_time:348696ms step_avg:425.24ms
step:831/1500 train_loss:3.6844 train_time:349113ms step_avg:425.23ms
step:832/1500 train_loss:3.7305 train_time:349531ms step_avg:425.22ms
step:833/1500 train_loss:3.6612 train_time:349947ms step_avg:425.21ms
step:834/1500 train_loss:3.7864 train_time:350363ms step_avg:425.20ms
step:835/1500 train_loss:3.6274 train_time:350779ms step_avg:425.19ms
step:836/1500 train_loss:3.6052 train_time:351196ms step_avg:425.18ms
step:837/1500 train_loss:3.8705 train_time:351611ms step_avg:425.17ms
step:838/1500 train_loss:3.5584 train_time:352031ms step_avg:425.16ms
step:839/1500 train_loss:3.7380 train_time:352448ms step_avg:425.15ms
step:840/1500 train_loss:3.5745 train_time:352865ms step_avg:425.14ms
step:841/1500 train_loss:3.6183 train_time:353279ms step_avg:425.13ms
step:842/1500 train_loss:3.7111 train_time:353696ms step_avg:425.11ms
step:843/1500 train_loss:3.7241 train_time:354111ms step_avg:425.10ms
step:844/1500 train_loss:3.7274 train_time:354530ms step_avg:425.10ms
step:845/1500 train_loss:3.5715 train_time:354946ms step_avg:425.08ms
step:846/1500 train_loss:3.8117 train_time:355361ms step_avg:425.07ms
step:847/1500 train_loss:3.6728 train_time:355777ms step_avg:425.06ms
step:848/1500 train_loss:3.6295 train_time:356192ms step_avg:425.05ms
step:849/1500 train_loss:3.7739 train_time:356608ms step_avg:425.04ms
step:850/1500 train_loss:3.6403 train_time:357025ms step_avg:425.03ms
step:851/1500 train_loss:3.5993 train_time:357442ms step_avg:425.02ms
step:852/1500 train_loss:3.8810 train_time:357859ms step_avg:425.01ms
step:853/1500 train_loss:3.5929 train_time:358276ms step_avg:425.00ms
step:854/1500 train_loss:3.7098 train_time:358691ms step_avg:424.99ms
step:855/1500 train_loss:3.7903 train_time:359108ms step_avg:424.98ms
step:856/1500 train_loss:3.6718 train_time:359524ms step_avg:424.97ms
step:857/1500 train_loss:3.6961 train_time:359941ms step_avg:424.96ms
step:858/1500 train_loss:3.7480 train_time:360358ms step_avg:424.95ms
step:859/1500 train_loss:3.6282 train_time:360775ms step_avg:424.94ms
step:860/1500 train_loss:3.7035 train_time:361190ms step_avg:424.93ms
step:861/1500 train_loss:3.7327 train_time:361607ms step_avg:424.92ms
step:862/1500 train_loss:3.7924 train_time:362023ms step_avg:424.91ms
step:863/1500 train_loss:3.7389 train_time:362440ms step_avg:424.90ms
step:864/1500 train_loss:3.7187 train_time:362855ms step_avg:424.89ms
step:865/1500 train_loss:3.5334 train_time:363272ms step_avg:424.88ms
step:866/1500 train_loss:3.7292 train_time:363689ms step_avg:424.87ms
step:867/1500 train_loss:4.0054 train_time:364104ms step_avg:424.86ms
step:868/1500 train_loss:3.5931 train_time:364521ms step_avg:424.85ms
step:869/1500 train_loss:3.7795 train_time:364938ms step_avg:424.84ms
step:870/1500 train_loss:3.7540 train_time:365354ms step_avg:424.83ms
step:871/1500 train_loss:3.5945 train_time:365773ms step_avg:424.82ms
step:872/1500 train_loss:3.5439 train_time:366188ms step_avg:424.81ms
step:873/1500 train_loss:3.8058 train_time:366604ms step_avg:424.80ms
step:874/1500 train_loss:3.5962 train_time:367020ms step_avg:424.79ms
step:875/1500 train_loss:3.3214 train_time:367436ms step_avg:424.78ms
step:875/1500 val_loss:3.6663 train_time:367449ms step_avg:424.80ms
step:876/1500 train_loss:3.7849 train_time:367856ms step_avg:424.78ms
step:877/1500 train_loss:3.5888 train_time:368271ms step_avg:424.77ms
step:878/1500 train_loss:3.7655 train_time:368688ms step_avg:424.76ms
step:879/1500 train_loss:3.6196 train_time:369104ms step_avg:424.75ms
step:880/1500 train_loss:3.8046 train_time:369521ms step_avg:424.74ms
step:881/1500 train_loss:3.4675 train_time:369938ms step_avg:424.73ms
step:882/1500 train_loss:3.6313 train_time:370355ms step_avg:424.72ms
step:883/1500 train_loss:3.8298 train_time:370771ms step_avg:424.71ms
step:884/1500 train_loss:3.9852 train_time:371187ms step_avg:424.70ms
step:885/1500 train_loss:3.7060 train_time:371603ms step_avg:424.69ms
step:886/1500 train_loss:3.6251 train_time:372020ms step_avg:424.68ms
step:887/1500 train_loss:3.7189 train_time:372437ms step_avg:424.67ms
step:888/1500 train_loss:4.2217 train_time:372853ms step_avg:424.66ms
step:889/1500 train_loss:3.9951 train_time:373271ms step_avg:424.65ms
step:890/1500 train_loss:3.6624 train_time:373688ms step_avg:424.65ms
step:891/1500 train_loss:3.6752 train_time:374104ms step_avg:424.64ms
step:892/1500 train_loss:3.5042 train_time:374519ms step_avg:424.62ms
step:893/1500 train_loss:3.8492 train_time:374934ms step_avg:424.61ms
step:894/1500 train_loss:3.5682 train_time:375351ms step_avg:424.60ms
step:895/1500 train_loss:3.8209 train_time:375765ms step_avg:424.59ms
step:896/1500 train_loss:3.8329 train_time:376182ms step_avg:424.58ms
step:897/1500 train_loss:3.6320 train_time:376600ms step_avg:424.58ms
step:898/1500 train_loss:3.6761 train_time:377016ms step_avg:424.57ms
step:899/1500 train_loss:3.7278 train_time:377433ms step_avg:424.56ms
step:900/1500 train_loss:3.6127 train_time:377849ms step_avg:424.55ms
step:901/1500 train_loss:3.5586 train_time:378265ms step_avg:424.54ms
step:902/1500 train_loss:3.7732 train_time:378683ms step_avg:424.53ms
step:903/1500 train_loss:3.7720 train_time:379103ms step_avg:424.53ms
step:904/1500 train_loss:3.6757 train_time:379523ms step_avg:424.52ms
step:905/1500 train_loss:3.6422 train_time:379939ms step_avg:424.51ms
step:906/1500 train_loss:3.6357 train_time:380355ms step_avg:424.50ms
step:907/1500 train_loss:3.8561 train_time:380773ms step_avg:424.50ms
step:908/1500 train_loss:3.6465 train_time:381190ms step_avg:424.49ms
step:909/1500 train_loss:3.6961 train_time:381606ms step_avg:424.48ms
step:910/1500 train_loss:3.6006 train_time:382022ms step_avg:424.47ms
step:911/1500 train_loss:3.6869 train_time:382438ms step_avg:424.46ms
step:912/1500 train_loss:3.7659 train_time:382859ms step_avg:424.46ms
step:913/1500 train_loss:3.7480 train_time:383274ms step_avg:424.45ms
step:914/1500 train_loss:3.6170 train_time:383691ms step_avg:424.44ms
step:915/1500 train_loss:3.8750 train_time:384107ms step_avg:424.43ms
step:916/1500 train_loss:3.6710 train_time:384526ms step_avg:424.42ms
step:917/1500 train_loss:3.7703 train_time:384942ms step_avg:424.41ms
step:918/1500 train_loss:3.7380 train_time:385357ms step_avg:424.40ms
step:919/1500 train_loss:4.9715 train_time:385773ms step_avg:424.39ms
step:920/1500 train_loss:3.6515 train_time:386191ms step_avg:424.39ms
step:921/1500 train_loss:3.7099 train_time:386607ms step_avg:424.38ms
step:922/1500 train_loss:3.6759 train_time:387023ms step_avg:424.37ms
step:923/1500 train_loss:3.7191 train_time:387439ms step_avg:424.36ms
step:924/1500 train_loss:3.7364 train_time:387855ms step_avg:424.35ms
step:925/1500 train_loss:3.8268 train_time:388271ms step_avg:424.34ms
step:926/1500 train_loss:3.7960 train_time:388688ms step_avg:424.33ms
step:927/1500 train_loss:3.6942 train_time:389104ms step_avg:424.32ms
step:928/1500 train_loss:3.6863 train_time:389521ms step_avg:424.32ms
step:929/1500 train_loss:3.9131 train_time:389939ms step_avg:424.31ms
step:930/1500 train_loss:3.7560 train_time:390356ms step_avg:424.30ms
step:931/1500 train_loss:3.5469 train_time:390771ms step_avg:424.29ms
step:932/1500 train_loss:3.6297 train_time:391187ms step_avg:424.28ms
step:933/1500 train_loss:3.8083 train_time:391602ms step_avg:424.27ms
step:934/1500 train_loss:3.5170 train_time:392019ms step_avg:424.26ms
step:935/1500 train_loss:3.7159 train_time:392435ms step_avg:424.25ms
step:936/1500 train_loss:3.5882 train_time:392853ms step_avg:424.25ms
step:937/1500 train_loss:3.6557 train_time:393267ms step_avg:424.24ms
step:938/1500 train_loss:3.7451 train_time:393683ms step_avg:424.23ms
step:939/1500 train_loss:3.6746 train_time:394101ms step_avg:424.22ms
step:940/1500 train_loss:3.8324 train_time:394517ms step_avg:424.21ms
step:941/1500 train_loss:3.6231 train_time:394934ms step_avg:424.20ms
step:942/1500 train_loss:3.6840 train_time:395349ms step_avg:424.19ms
step:943/1500 train_loss:3.4894 train_time:395766ms step_avg:424.19ms
step:944/1500 train_loss:3.8420 train_time:396183ms step_avg:424.18ms
step:945/1500 train_loss:3.5507 train_time:397495ms step_avg:425.13ms
step:946/1500 train_loss:3.5591 train_time:397912ms step_avg:425.12ms
step:947/1500 train_loss:5.1836 train_time:398329ms step_avg:425.11ms
step:948/1500 train_loss:3.7406 train_time:398743ms step_avg:425.10ms
step:949/1500 train_loss:3.6346 train_time:399159ms step_avg:425.09ms
step:950/1500 train_loss:3.5328 train_time:399700ms step_avg:425.21ms
step:951/1500 train_loss:3.5911 train_time:400116ms step_avg:425.20ms
step:952/1500 train_loss:3.5405 train_time:400533ms step_avg:425.19ms
step:953/1500 train_loss:3.6132 train_time:400947ms step_avg:425.18ms
step:954/1500 train_loss:3.6923 train_time:401364ms step_avg:425.17ms
step:955/1500 train_loss:3.5772 train_time:401780ms step_avg:425.16ms
step:956/1500 train_loss:3.6122 train_time:402200ms step_avg:425.16ms
step:957/1500 train_loss:3.5749 train_time:402616ms step_avg:425.15ms
step:958/1500 train_loss:3.6354 train_time:403031ms step_avg:425.14ms
step:959/1500 train_loss:3.6296 train_time:403447ms step_avg:425.13ms
step:960/1500 train_loss:3.6458 train_time:403864ms step_avg:425.12ms
step:961/1500 train_loss:3.5302 train_time:404280ms step_avg:425.11ms
step:962/1500 train_loss:3.7879 train_time:404699ms step_avg:425.10ms
step:963/1500 train_loss:3.7344 train_time:405116ms step_avg:425.10ms
step:964/1500 train_loss:3.5848 train_time:405532ms step_avg:425.09ms
step:965/1500 train_loss:3.5848 train_time:405948ms step_avg:425.08ms
step:966/1500 train_loss:3.6190 train_time:406364ms step_avg:425.07ms
step:967/1500 train_loss:3.8416 train_time:406779ms step_avg:425.06ms
step:968/1500 train_loss:3.6697 train_time:407199ms step_avg:425.05ms
step:969/1500 train_loss:3.6514 train_time:407615ms step_avg:425.04ms
step:970/1500 train_loss:3.7127 train_time:408032ms step_avg:425.03ms
step:971/1500 train_loss:3.5284 train_time:408447ms step_avg:425.02ms
step:972/1500 train_loss:3.6797 train_time:408865ms step_avg:425.02ms
step:973/1500 train_loss:3.6272 train_time:409280ms step_avg:425.01ms
step:974/1500 train_loss:3.6789 train_time:409699ms step_avg:425.00ms
step:975/1500 train_loss:3.7476 train_time:410113ms step_avg:424.99ms
step:976/1500 train_loss:3.6197 train_time:410529ms step_avg:424.98ms
step:977/1500 train_loss:3.8190 train_time:410945ms step_avg:424.97ms
step:978/1500 train_loss:3.7048 train_time:411361ms step_avg:424.96ms
step:979/1500 train_loss:3.5196 train_time:411780ms step_avg:424.95ms
step:980/1500 train_loss:3.8152 train_time:412200ms step_avg:424.95ms
step:981/1500 train_loss:3.5563 train_time:412616ms step_avg:424.94ms
step:982/1500 train_loss:3.7191 train_time:413033ms step_avg:424.93ms
step:983/1500 train_loss:3.6937 train_time:413449ms step_avg:424.92ms
step:984/1500 train_loss:3.6962 train_time:413865ms step_avg:424.91ms
step:985/1500 train_loss:3.6535 train_time:414281ms step_avg:424.90ms
step:986/1500 train_loss:3.7258 train_time:414700ms step_avg:424.90ms
step:987/1500 train_loss:3.5495 train_time:415115ms step_avg:424.89ms
step:988/1500 train_loss:3.6289 train_time:415531ms step_avg:424.88ms
step:989/1500 train_loss:3.6151 train_time:415948ms step_avg:424.87ms
step:990/1500 train_loss:3.5693 train_time:416364ms step_avg:424.86ms
step:991/1500 train_loss:3.7844 train_time:416781ms step_avg:424.85ms
step:992/1500 train_loss:3.6071 train_time:417199ms step_avg:424.85ms
step:993/1500 train_loss:3.5837 train_time:417615ms step_avg:424.84ms
step:994/1500 train_loss:3.6495 train_time:418033ms step_avg:424.83ms
step:995/1500 train_loss:3.7379 train_time:418449ms step_avg:424.82ms
step:996/1500 train_loss:3.6801 train_time:418865ms step_avg:424.81ms
step:997/1500 train_loss:3.5934 train_time:419281ms step_avg:424.80ms
step:998/1500 train_loss:3.9333 train_time:419701ms step_avg:424.80ms
step:999/1500 train_loss:3.5991 train_time:420117ms step_avg:424.79ms
step:1000/1500 train_loss:3.7231 train_time:420534ms step_avg:424.78ms
step:1000/1500 val_loss:3.6199 train_time:420547ms step_avg:424.79ms
step:1001/1500 train_loss:3.5959 train_time:420953ms step_avg:424.78ms
step:1002/1500 train_loss:3.6419 train_time:421369ms step_avg:424.77ms
step:1003/1500 train_loss:3.5286 train_time:421786ms step_avg:424.76ms
step:1004/1500 train_loss:3.7131 train_time:422202ms step_avg:424.75ms
step:1005/1500 train_loss:3.7591 train_time:422617ms step_avg:424.74ms
step:1006/1500 train_loss:3.5364 train_time:423034ms step_avg:424.73ms
step:1007/1500 train_loss:3.6202 train_time:423450ms step_avg:424.72ms
step:1008/1500 train_loss:3.5867 train_time:423866ms step_avg:424.72ms
step:1009/1500 train_loss:3.7050 train_time:424282ms step_avg:424.71ms
step:1010/1500 train_loss:3.8110 train_time:424698ms step_avg:424.70ms
step:1011/1500 train_loss:3.6986 train_time:425115ms step_avg:424.69ms
step:1012/1500 train_loss:3.6686 train_time:425530ms step_avg:424.68ms
step:1013/1500 train_loss:3.5336 train_time:425946ms step_avg:424.67ms
step:1014/1500 train_loss:3.6704 train_time:426366ms step_avg:424.67ms
step:1015/1500 train_loss:3.7849 train_time:426782ms step_avg:424.66ms
step:1016/1500 train_loss:3.4916 train_time:427198ms step_avg:424.65ms
step:1017/1500 train_loss:3.5769 train_time:427613ms step_avg:424.64ms
step:1018/1500 train_loss:3.5776 train_time:428029ms step_avg:424.63ms
step:1019/1500 train_loss:3.5301 train_time:428446ms step_avg:424.62ms
step:1020/1500 train_loss:3.6676 train_time:428866ms step_avg:424.62ms
step:1021/1500 train_loss:3.5773 train_time:429282ms step_avg:424.61ms
step:1022/1500 train_loss:3.5152 train_time:429699ms step_avg:424.60ms
step:1023/1500 train_loss:3.6190 train_time:430115ms step_avg:424.60ms
step:1024/1500 train_loss:3.6494 train_time:430532ms step_avg:424.59ms
step:1025/1500 train_loss:3.6308 train_time:430949ms step_avg:424.58ms
step:1026/1500 train_loss:3.6368 train_time:431368ms step_avg:424.57ms
step:1027/1500 train_loss:3.7923 train_time:431786ms step_avg:424.57ms
step:1028/1500 train_loss:3.4857 train_time:432204ms step_avg:424.56ms
step:1029/1500 train_loss:3.5433 train_time:432621ms step_avg:424.55ms
step:1030/1500 train_loss:3.4867 train_time:433036ms step_avg:424.55ms
step:1031/1500 train_loss:3.6661 train_time:433453ms step_avg:424.54ms
step:1032/1500 train_loss:3.6476 train_time:433869ms step_avg:424.53ms
step:1033/1500 train_loss:3.8303 train_time:434283ms step_avg:424.52ms
step:1034/1500 train_loss:3.6442 train_time:434700ms step_avg:424.51ms
step:1035/1500 train_loss:3.5532 train_time:435117ms step_avg:424.50ms
step:1036/1500 train_loss:3.5838 train_time:435533ms step_avg:424.50ms
step:1037/1500 train_loss:3.6413 train_time:435951ms step_avg:424.49ms
step:1038/1500 train_loss:3.9501 train_time:436367ms step_avg:424.48ms
step:1039/1500 train_loss:3.7673 train_time:436783ms step_avg:424.47ms
step:1040/1500 train_loss:3.6656 train_time:437200ms step_avg:424.47ms
step:1041/1500 train_loss:3.5612 train_time:437619ms step_avg:424.46ms
step:1042/1500 train_loss:3.6369 train_time:438036ms step_avg:424.45ms
step:1043/1500 train_loss:3.6677 train_time:438453ms step_avg:424.45ms
step:1044/1500 train_loss:3.6005 train_time:438870ms step_avg:424.44ms
step:1045/1500 train_loss:3.6123 train_time:439287ms step_avg:424.43ms
step:1046/1500 train_loss:3.6868 train_time:439704ms step_avg:424.42ms
step:1047/1500 train_loss:3.5894 train_time:440120ms step_avg:424.42ms
step:1048/1500 train_loss:3.7973 train_time:440534ms step_avg:424.41ms
step:1049/1500 train_loss:3.6461 train_time:440953ms step_avg:424.40ms
step:1050/1500 train_loss:3.5726 train_time:441368ms step_avg:424.39ms
step:1051/1500 train_loss:3.5409 train_time:441786ms step_avg:424.39ms
step:1052/1500 train_loss:3.6632 train_time:442204ms step_avg:424.38ms
step:1053/1500 train_loss:3.5397 train_time:442619ms step_avg:424.37ms
step:1054/1500 train_loss:3.8581 train_time:443034ms step_avg:424.36ms
step:1055/1500 train_loss:3.6933 train_time:443449ms step_avg:424.35ms
step:1056/1500 train_loss:3.5590 train_time:443866ms step_avg:424.35ms
step:1057/1500 train_loss:3.6573 train_time:444284ms step_avg:424.34ms
step:1058/1500 train_loss:3.7338 train_time:444700ms step_avg:424.33ms
step:1059/1500 train_loss:3.4494 train_time:445115ms step_avg:424.32ms
step:1060/1500 train_loss:3.5725 train_time:445531ms step_avg:424.32ms
step:1061/1500 train_loss:3.5944 train_time:445949ms step_avg:424.31ms
step:1062/1500 train_loss:3.5681 train_time:446367ms step_avg:424.30ms
step:1063/1500 train_loss:3.5433 train_time:446784ms step_avg:424.30ms
step:1064/1500 train_loss:3.6422 train_time:447200ms step_avg:424.29ms
step:1065/1500 train_loss:3.5416 train_time:447615ms step_avg:424.28ms
step:1066/1500 train_loss:3.5333 train_time:448030ms step_avg:424.27ms
step:1067/1500 train_loss:3.5575 train_time:448448ms step_avg:424.26ms
step:1068/1500 train_loss:3.4683 train_time:448866ms step_avg:424.26ms
step:1069/1500 train_loss:3.5825 train_time:449282ms step_avg:424.25ms
step:1070/1500 train_loss:3.4469 train_time:449699ms step_avg:424.24ms
step:1071/1500 train_loss:3.7107 train_time:450114ms step_avg:424.24ms
step:1072/1500 train_loss:3.6625 train_time:450529ms step_avg:424.23ms
step:1073/1500 train_loss:3.6112 train_time:450946ms step_avg:424.22ms
step:1074/1500 train_loss:3.6710 train_time:451366ms step_avg:424.22ms
step:1075/1500 train_loss:3.6167 train_time:451783ms step_avg:424.21ms
step:1076/1500 train_loss:3.5617 train_time:452200ms step_avg:424.20ms
step:1077/1500 train_loss:3.9560 train_time:452617ms step_avg:424.20ms
step:1078/1500 train_loss:3.6188 train_time:453033ms step_avg:424.19ms
step:1079/1500 train_loss:3.3439 train_time:453450ms step_avg:424.18ms
step:1080/1500 train_loss:3.6907 train_time:453867ms step_avg:424.17ms
step:1081/1500 train_loss:3.6079 train_time:454281ms step_avg:424.17ms
step:1082/1500 train_loss:3.6641 train_time:454698ms step_avg:424.16ms
step:1083/1500 train_loss:3.7656 train_time:455114ms step_avg:424.15ms
step:1084/1500 train_loss:3.6633 train_time:455531ms step_avg:424.14ms
step:1085/1500 train_loss:3.6328 train_time:455947ms step_avg:424.14ms
step:1086/1500 train_loss:3.6043 train_time:456366ms step_avg:424.13ms
step:1087/1500 train_loss:3.7930 train_time:456782ms step_avg:424.12ms
step:1088/1500 train_loss:3.6806 train_time:457200ms step_avg:424.12ms
step:1089/1500 train_loss:3.5181 train_time:457616ms step_avg:424.11ms
step:1090/1500 train_loss:3.5445 train_time:458033ms step_avg:424.10ms
step:1091/1500 train_loss:3.6577 train_time:458451ms step_avg:424.10ms
step:1092/1500 train_loss:3.4512 train_time:458868ms step_avg:424.09ms
step:1093/1500 train_loss:3.6467 train_time:459284ms step_avg:424.08ms
step:1094/1500 train_loss:3.7882 train_time:459942ms step_avg:424.30ms
step:1095/1500 train_loss:3.6221 train_time:460358ms step_avg:424.29ms
step:1096/1500 train_loss:3.5726 train_time:460775ms step_avg:424.29ms
step:1097/1500 train_loss:3.5947 train_time:461189ms step_avg:424.28ms
step:1098/1500 train_loss:3.6441 train_time:461605ms step_avg:424.27ms
step:1099/1500 train_loss:3.7191 train_time:462022ms step_avg:424.26ms
step:1100/1500 train_loss:3.6817 train_time:462439ms step_avg:424.26ms
step:1101/1500 train_loss:3.6004 train_time:462856ms step_avg:424.25ms
step:1102/1500 train_loss:3.4645 train_time:463273ms step_avg:424.24ms
step:1103/1500 train_loss:3.5448 train_time:463688ms step_avg:424.23ms
step:1104/1500 train_loss:3.6097 train_time:464104ms step_avg:424.23ms
step:1105/1500 train_loss:3.4916 train_time:464518ms step_avg:424.22ms
step:1106/1500 train_loss:4.2502 train_time:464936ms step_avg:424.21ms
step:1107/1500 train_loss:3.3964 train_time:465353ms step_avg:424.20ms
step:1108/1500 train_loss:3.7353 train_time:465769ms step_avg:424.20ms
step:1109/1500 train_loss:3.5211 train_time:466185ms step_avg:424.19ms
step:1110/1500 train_loss:3.6642 train_time:466601ms step_avg:424.18ms
step:1111/1500 train_loss:3.5922 train_time:467018ms step_avg:424.18ms
step:1112/1500 train_loss:3.6403 train_time:467434ms step_avg:424.17ms
step:1113/1500 train_loss:3.7409 train_time:467850ms step_avg:424.16ms
step:1114/1500 train_loss:3.5883 train_time:468267ms step_avg:424.15ms
step:1115/1500 train_loss:3.5431 train_time:468682ms step_avg:424.15ms
step:1116/1500 train_loss:3.4296 train_time:469099ms step_avg:424.14ms
step:1117/1500 train_loss:3.6054 train_time:469515ms step_avg:424.13ms
step:1118/1500 train_loss:3.7482 train_time:469931ms step_avg:424.13ms
step:1119/1500 train_loss:3.7935 train_time:470346ms step_avg:424.12ms
step:1120/1500 train_loss:3.6307 train_time:470767ms step_avg:424.11ms
step:1121/1500 train_loss:3.6600 train_time:471182ms step_avg:424.11ms
step:1122/1500 train_loss:3.5537 train_time:471597ms step_avg:424.10ms
step:1123/1500 train_loss:3.6179 train_time:472014ms step_avg:424.09ms
step:1124/1500 train_loss:3.7532 train_time:472430ms step_avg:424.08ms
step:1125/1500 train_loss:3.5205 train_time:472847ms step_avg:424.08ms
step:1125/1500 val_loss:3.5828 train_time:472866ms step_avg:424.10ms
step:1126/1500 train_loss:3.4133 train_time:473273ms step_avg:424.08ms
step:1127/1500 train_loss:3.6371 train_time:473688ms step_avg:424.07ms
step:1128/1500 train_loss:3.8592 train_time:474105ms step_avg:424.07ms
step:1129/1500 train_loss:3.4003 train_time:474520ms step_avg:424.06ms
step:1130/1500 train_loss:3.7231 train_time:474939ms step_avg:424.05ms
step:1131/1500 train_loss:3.5509 train_time:475355ms step_avg:424.05ms
step:1132/1500 train_loss:3.5789 train_time:475772ms step_avg:424.04ms
step:1133/1500 train_loss:3.5386 train_time:476188ms step_avg:424.03ms
step:1134/1500 train_loss:3.6886 train_time:477522ms step_avg:424.84ms
step:1135/1500 train_loss:3.6296 train_time:477938ms step_avg:424.83ms
step:1136/1500 train_loss:3.6801 train_time:478355ms step_avg:424.83ms
step:1137/1500 train_loss:3.7180 train_time:478770ms step_avg:424.82ms
step:1138/1500 train_loss:3.6298 train_time:479186ms step_avg:424.81ms
step:1139/1500 train_loss:3.5294 train_time:479602ms step_avg:424.80ms
step:1140/1500 train_loss:3.8354 train_time:480147ms step_avg:424.91ms
step:1141/1500 train_loss:3.6327 train_time:480564ms step_avg:424.90ms
step:1142/1500 train_loss:3.7308 train_time:480979ms step_avg:424.89ms
step:1143/1500 train_loss:3.6215 train_time:481395ms step_avg:424.89ms
step:1144/1500 train_loss:3.5350 train_time:481812ms step_avg:424.88ms
step:1145/1500 train_loss:3.6388 train_time:482229ms step_avg:424.87ms
step:1146/1500 train_loss:3.7573 train_time:482647ms step_avg:424.87ms
step:1147/1500 train_loss:3.7325 train_time:483063ms step_avg:424.86ms
step:1148/1500 train_loss:3.6461 train_time:483480ms step_avg:424.85ms
step:1149/1500 train_loss:3.6647 train_time:483897ms step_avg:424.84ms
step:1150/1500 train_loss:3.5212 train_time:484312ms step_avg:424.84ms
step:1151/1500 train_loss:3.5477 train_time:484730ms step_avg:424.83ms
step:1152/1500 train_loss:3.5019 train_time:485146ms step_avg:424.82ms
step:1153/1500 train_loss:3.6522 train_time:485563ms step_avg:424.81ms
step:1154/1500 train_loss:3.6212 train_time:485979ms step_avg:424.81ms
step:1155/1500 train_loss:3.6889 train_time:486395ms step_avg:424.80ms
step:1156/1500 train_loss:3.5296 train_time:486809ms step_avg:424.79ms
step:1157/1500 train_loss:3.7061 train_time:487226ms step_avg:424.78ms
step:1158/1500 train_loss:3.6601 train_time:487642ms step_avg:424.78ms
step:1159/1500 train_loss:3.4685 train_time:488059ms step_avg:424.77ms
step:1160/1500 train_loss:3.5100 train_time:488475ms step_avg:424.76ms
step:1161/1500 train_loss:3.4942 train_time:488891ms step_avg:424.75ms
step:1162/1500 train_loss:3.2984 train_time:489310ms step_avg:424.75ms
step:1163/1500 train_loss:3.6115 train_time:489726ms step_avg:424.74ms
step:1164/1500 train_loss:3.5811 train_time:490142ms step_avg:424.73ms
step:1165/1500 train_loss:3.4476 train_time:490558ms step_avg:424.73ms
step:1166/1500 train_loss:3.4386 train_time:490974ms step_avg:424.72ms
step:1167/1500 train_loss:3.5546 train_time:491391ms step_avg:424.71ms
step:1168/1500 train_loss:3.5655 train_time:491807ms step_avg:424.70ms
step:1169/1500 train_loss:3.8819 train_time:492224ms step_avg:424.70ms
step:1170/1500 train_loss:3.5593 train_time:492640ms step_avg:424.69ms
step:1171/1500 train_loss:3.5753 train_time:493056ms step_avg:424.68ms
step:1172/1500 train_loss:3.4608 train_time:493473ms step_avg:424.68ms
step:1173/1500 train_loss:3.5800 train_time:493888ms step_avg:424.67ms
step:1174/1500 train_loss:3.7133 train_time:494305ms step_avg:424.66ms
step:1175/1500 train_loss:3.5585 train_time:494722ms step_avg:424.65ms
step:1176/1500 train_loss:3.5755 train_time:495138ms step_avg:424.65ms
step:1177/1500 train_loss:3.6246 train_time:495556ms step_avg:424.64ms
step:1178/1500 train_loss:3.6121 train_time:495971ms step_avg:424.63ms
step:1179/1500 train_loss:3.6675 train_time:496387ms step_avg:424.62ms
step:1180/1500 train_loss:3.5679 train_time:496803ms step_avg:424.62ms
step:1181/1500 train_loss:3.5829 train_time:497220ms step_avg:424.61ms
step:1182/1500 train_loss:3.5185 train_time:497638ms step_avg:424.61ms
step:1183/1500 train_loss:3.5601 train_time:498054ms step_avg:424.60ms
step:1184/1500 train_loss:3.5115 train_time:498471ms step_avg:424.59ms
step:1185/1500 train_loss:3.6766 train_time:498888ms step_avg:424.59ms
step:1186/1500 train_loss:3.7379 train_time:499302ms step_avg:424.58ms
step:1187/1500 train_loss:3.5335 train_time:499718ms step_avg:424.57ms
step:1188/1500 train_loss:3.5872 train_time:500137ms step_avg:424.56ms
step:1189/1500 train_loss:3.6074 train_time:500553ms step_avg:424.56ms
step:1190/1500 train_loss:3.4510 train_time:500970ms step_avg:424.55ms
step:1191/1500 train_loss:3.6308 train_time:501387ms step_avg:424.54ms
step:1192/1500 train_loss:3.7710 train_time:501804ms step_avg:424.54ms
step:1193/1500 train_loss:3.5704 train_time:502222ms step_avg:424.53ms
step:1194/1500 train_loss:3.4521 train_time:502639ms step_avg:424.53ms
step:1195/1500 train_loss:3.7534 train_time:503055ms step_avg:424.52ms
step:1196/1500 train_loss:3.5476 train_time:503471ms step_avg:424.51ms
step:1197/1500 train_loss:3.5581 train_time:503887ms step_avg:424.50ms
step:1198/1500 train_loss:3.4600 train_time:504301ms step_avg:424.50ms
step:1199/1500 train_loss:3.4712 train_time:504718ms step_avg:424.49ms
step:1200/1500 train_loss:3.5241 train_time:505139ms step_avg:424.49ms
step:1201/1500 train_loss:3.6125 train_time:505555ms step_avg:424.48ms
step:1202/1500 train_loss:3.6800 train_time:505972ms step_avg:424.47ms
step:1203/1500 train_loss:3.7143 train_time:506388ms step_avg:424.47ms
step:1204/1500 train_loss:3.5904 train_time:506804ms step_avg:424.46ms
step:1205/1500 train_loss:3.5029 train_time:507221ms step_avg:424.45ms
step:1206/1500 train_loss:3.6089 train_time:507638ms step_avg:424.45ms
step:1207/1500 train_loss:3.6477 train_time:508054ms step_avg:424.44ms
step:1208/1500 train_loss:3.6941 train_time:508470ms step_avg:424.43ms
step:1209/1500 train_loss:3.5770 train_time:508886ms step_avg:424.43ms
step:1210/1500 train_loss:3.4363 train_time:509303ms step_avg:424.42ms
step:1211/1500 train_loss:3.4877 train_time:509720ms step_avg:424.41ms
step:1212/1500 train_loss:3.5805 train_time:510139ms step_avg:424.41ms
step:1213/1500 train_loss:3.5913 train_time:510555ms step_avg:424.40ms
step:1214/1500 train_loss:3.6217 train_time:510971ms step_avg:424.39ms
step:1215/1500 train_loss:3.4967 train_time:511388ms step_avg:424.39ms
step:1216/1500 train_loss:3.5802 train_time:511804ms step_avg:424.38ms
step:1217/1500 train_loss:3.5183 train_time:512220ms step_avg:424.37ms
step:1218/1500 train_loss:3.5098 train_time:512640ms step_avg:424.37ms
step:1219/1500 train_loss:3.5979 train_time:513057ms step_avg:424.36ms
step:1220/1500 train_loss:3.4423 train_time:513473ms step_avg:424.36ms
step:1221/1500 train_loss:3.6736 train_time:513889ms step_avg:424.35ms
step:1222/1500 train_loss:3.6964 train_time:514306ms step_avg:424.34ms
step:1223/1500 train_loss:3.6205 train_time:514721ms step_avg:424.34ms
step:1224/1500 train_loss:3.4791 train_time:515137ms step_avg:424.33ms
step:1225/1500 train_loss:3.4668 train_time:515553ms step_avg:424.32ms
step:1226/1500 train_loss:3.5467 train_time:515970ms step_avg:424.32ms
step:1227/1500 train_loss:3.5300 train_time:516385ms step_avg:424.31ms
step:1228/1500 train_loss:3.4690 train_time:516802ms step_avg:424.30ms
step:1229/1500 train_loss:3.6443 train_time:517217ms step_avg:424.30ms
step:1230/1500 train_loss:3.5610 train_time:517637ms step_avg:424.29ms
step:1231/1500 train_loss:3.6164 train_time:518054ms step_avg:424.29ms
step:1232/1500 train_loss:3.7719 train_time:518471ms step_avg:424.28ms
step:1233/1500 train_loss:3.6748 train_time:518887ms step_avg:424.27ms
step:1234/1500 train_loss:3.6091 train_time:519303ms step_avg:424.27ms
step:1235/1500 train_loss:3.7599 train_time:519720ms step_avg:424.26ms
step:1236/1500 train_loss:3.5217 train_time:520139ms step_avg:424.26ms
step:1237/1500 train_loss:3.4903 train_time:520556ms step_avg:424.25ms
step:1238/1500 train_loss:3.4394 train_time:520971ms step_avg:424.24ms
step:1239/1500 train_loss:3.5103 train_time:521388ms step_avg:424.24ms
step:1240/1500 train_loss:3.5273 train_time:521804ms step_avg:424.23ms
step:1241/1500 train_loss:3.5672 train_time:522218ms step_avg:424.22ms
step:1242/1500 train_loss:3.6221 train_time:522639ms step_avg:424.22ms
step:1243/1500 train_loss:3.4844 train_time:523055ms step_avg:424.21ms
step:1244/1500 train_loss:3.5830 train_time:523472ms step_avg:424.21ms
step:1245/1500 train_loss:3.5950 train_time:523889ms step_avg:424.20ms
step:1246/1500 train_loss:3.6015 train_time:524306ms step_avg:424.20ms
step:1247/1500 train_loss:3.4282 train_time:524721ms step_avg:424.19ms
step:1248/1500 train_loss:3.5731 train_time:525137ms step_avg:424.18ms
step:1249/1500 train_loss:3.6239 train_time:525554ms step_avg:424.18ms
step:1250/1500 train_loss:3.6045 train_time:525970ms step_avg:424.17ms
step:1250/1500 val_loss:3.5496 train_time:525983ms step_avg:424.18ms
step:1251/1500 train_loss:3.5021 train_time:526392ms step_avg:424.17ms
step:1252/1500 train_loss:3.7058 train_time:526807ms step_avg:424.16ms
step:1253/1500 train_loss:3.5652 train_time:527223ms step_avg:424.15ms
step:1254/1500 train_loss:3.5027 train_time:527640ms step_avg:424.15ms
step:1255/1500 train_loss:3.6327 train_time:528056ms step_avg:424.14ms
step:1256/1500 train_loss:3.6975 train_time:528472ms step_avg:424.13ms
step:1257/1500 train_loss:3.5028 train_time:528889ms step_avg:424.13ms
step:1258/1500 train_loss:3.5434 train_time:529305ms step_avg:424.12ms
step:1259/1500 train_loss:3.5862 train_time:529720ms step_avg:424.12ms
step:1260/1500 train_loss:3.5264 train_time:530137ms step_avg:424.11ms
step:1261/1500 train_loss:3.4006 train_time:530553ms step_avg:424.10ms
step:1262/1500 train_loss:3.4958 train_time:530969ms step_avg:424.10ms
step:1263/1500 train_loss:3.5706 train_time:531385ms step_avg:424.09ms
step:1264/1500 train_loss:3.4144 train_time:531805ms step_avg:424.09ms
step:1265/1500 train_loss:3.6291 train_time:532221ms step_avg:424.08ms
step:1266/1500 train_loss:3.6121 train_time:532635ms step_avg:424.07ms
step:1267/1500 train_loss:3.6187 train_time:533051ms step_avg:424.07ms
step:1268/1500 train_loss:3.5618 train_time:533467ms step_avg:424.06ms
step:1269/1500 train_loss:3.5972 train_time:533883ms step_avg:424.05ms
step:1270/1500 train_loss:3.4525 train_time:534302ms step_avg:424.05ms
step:1271/1500 train_loss:3.3033 train_time:534717ms step_avg:424.04ms
step:1272/1500 train_loss:3.5858 train_time:535143ms step_avg:424.04ms
step:1273/1500 train_loss:3.5389 train_time:535558ms step_avg:424.04ms
step:1274/1500 train_loss:3.5910 train_time:535973ms step_avg:424.03ms
step:1275/1500 train_loss:3.5444 train_time:536390ms step_avg:424.02ms
step:1276/1500 train_loss:3.6405 train_time:536807ms step_avg:424.02ms
step:1277/1500 train_loss:3.6581 train_time:537223ms step_avg:424.01ms
step:1278/1500 train_loss:3.6218 train_time:537639ms step_avg:424.01ms
step:1279/1500 train_loss:3.6094 train_time:538056ms step_avg:424.00ms
step:1280/1500 train_loss:3.4482 train_time:538472ms step_avg:423.99ms
step:1281/1500 train_loss:3.5526 train_time:538888ms step_avg:423.99ms
step:1282/1500 train_loss:3.6250 train_time:539306ms step_avg:423.98ms
step:1283/1500 train_loss:3.6568 train_time:539722ms step_avg:423.98ms
step:1284/1500 train_loss:3.5480 train_time:540138ms step_avg:423.97ms
step:1285/1500 train_loss:3.5703 train_time:540556ms step_avg:423.97ms
step:1286/1500 train_loss:3.5598 train_time:540973ms step_avg:423.96ms
step:1287/1500 train_loss:3.5330 train_time:541390ms step_avg:423.95ms
step:1288/1500 train_loss:3.6665 train_time:541807ms step_avg:423.95ms
step:1289/1500 train_loss:3.4927 train_time:542223ms step_avg:423.94ms
step:1290/1500 train_loss:3.5793 train_time:542639ms step_avg:423.94ms
step:1291/1500 train_loss:3.6544 train_time:543056ms step_avg:423.93ms
step:1292/1500 train_loss:3.5824 train_time:543471ms step_avg:423.92ms
step:1293/1500 train_loss:3.6825 train_time:543887ms step_avg:423.92ms
step:1294/1500 train_loss:3.7000 train_time:544306ms step_avg:423.91ms
step:1295/1500 train_loss:3.6636 train_time:544722ms step_avg:423.91ms
step:1296/1500 train_loss:3.4706 train_time:545139ms step_avg:423.90ms
step:1297/1500 train_loss:3.5568 train_time:545555ms step_avg:423.90ms
step:1298/1500 train_loss:3.4542 train_time:545972ms step_avg:423.89ms
step:1299/1500 train_loss:3.5216 train_time:546389ms step_avg:423.89ms
step:1300/1500 train_loss:3.5964 train_time:546805ms step_avg:423.88ms
step:1301/1500 train_loss:3.6052 train_time:547220ms step_avg:423.87ms
step:1302/1500 train_loss:3.6083 train_time:547638ms step_avg:423.87ms
step:1303/1500 train_loss:3.7615 train_time:548054ms step_avg:423.86ms
step:1304/1500 train_loss:3.5358 train_time:548470ms step_avg:423.86ms
step:1305/1500 train_loss:3.7316 train_time:548886ms step_avg:423.85ms
step:1306/1500 train_loss:3.4613 train_time:549304ms step_avg:423.85ms
step:1307/1500 train_loss:3.6567 train_time:549721ms step_avg:423.84ms
step:1308/1500 train_loss:3.6580 train_time:550138ms step_avg:423.83ms
step:1309/1500 train_loss:3.5171 train_time:550555ms step_avg:423.83ms
step:1310/1500 train_loss:3.4881 train_time:550971ms step_avg:423.82ms
step:1311/1500 train_loss:3.4850 train_time:551387ms step_avg:423.82ms
step:1312/1500 train_loss:3.4864 train_time:551805ms step_avg:423.81ms
step:1313/1500 train_loss:3.5971 train_time:552222ms step_avg:423.81ms
step:1314/1500 train_loss:3.5452 train_time:552638ms step_avg:423.80ms
step:1315/1500 train_loss:3.2613 train_time:553053ms step_avg:423.80ms
step:1316/1500 train_loss:3.4975 train_time:553470ms step_avg:423.79ms
step:1317/1500 train_loss:3.5802 train_time:553885ms step_avg:423.78ms
step:1318/1500 train_loss:3.6050 train_time:554306ms step_avg:423.78ms
step:1319/1500 train_loss:3.4921 train_time:554721ms step_avg:423.77ms
step:1320/1500 train_loss:3.6184 train_time:555138ms step_avg:423.77ms
step:1321/1500 train_loss:3.6696 train_time:555554ms step_avg:423.76ms
step:1322/1500 train_loss:3.5607 train_time:555971ms step_avg:423.76ms
step:1323/1500 train_loss:3.5097 train_time:557356ms step_avg:424.49ms
step:1324/1500 train_loss:3.5352 train_time:557773ms step_avg:424.49ms
step:1325/1500 train_loss:3.6300 train_time:558189ms step_avg:424.48ms
step:1326/1500 train_loss:3.6901 train_time:558607ms step_avg:424.47ms
step:1327/1500 train_loss:3.4352 train_time:559023ms step_avg:424.47ms
step:1328/1500 train_loss:3.3639 train_time:559437ms step_avg:424.46ms
step:1329/1500 train_loss:3.6809 train_time:559855ms step_avg:424.45ms
step:1330/1500 train_loss:3.5243 train_time:560409ms step_avg:424.55ms
step:1331/1500 train_loss:3.6418 train_time:560825ms step_avg:424.55ms
step:1332/1500 train_loss:3.5450 train_time:561240ms step_avg:424.54ms
step:1333/1500 train_loss:3.9413 train_time:561658ms step_avg:424.53ms
step:1334/1500 train_loss:3.6556 train_time:562074ms step_avg:424.53ms
step:1335/1500 train_loss:3.5598 train_time:562490ms step_avg:424.52ms
step:1336/1500 train_loss:3.5038 train_time:562907ms step_avg:424.52ms
step:1337/1500 train_loss:3.5007 train_time:563323ms step_avg:424.51ms
step:1338/1500 train_loss:3.7617 train_time:563740ms step_avg:424.50ms
step:1339/1500 train_loss:3.6952 train_time:564157ms step_avg:424.50ms
step:1340/1500 train_loss:3.5384 train_time:564574ms step_avg:424.49ms
step:1341/1500 train_loss:3.4959 train_time:564994ms step_avg:424.49ms
step:1342/1500 train_loss:3.8007 train_time:565411ms step_avg:424.48ms
step:1343/1500 train_loss:3.5706 train_time:565828ms step_avg:424.48ms
step:1344/1500 train_loss:3.5706 train_time:566246ms step_avg:424.47ms
step:1345/1500 train_loss:3.6255 train_time:566662ms step_avg:424.47ms
step:1346/1500 train_loss:3.5885 train_time:567079ms step_avg:424.46ms
step:1347/1500 train_loss:3.4918 train_time:567495ms step_avg:424.45ms
step:1348/1500 train_loss:3.4548 train_time:567911ms step_avg:424.45ms
step:1349/1500 train_loss:3.5439 train_time:568326ms step_avg:424.44ms
step:1350/1500 train_loss:3.4707 train_time:568742ms step_avg:424.43ms
step:1351/1500 train_loss:3.6002 train_time:569157ms step_avg:424.43ms
step:1352/1500 train_loss:3.4558 train_time:569574ms step_avg:424.42ms
step:1353/1500 train_loss:3.5131 train_time:569991ms step_avg:424.42ms
step:1354/1500 train_loss:3.6128 train_time:570407ms step_avg:424.41ms
step:1355/1500 train_loss:3.4613 train_time:570824ms step_avg:424.40ms
step:1356/1500 train_loss:3.3828 train_time:571240ms step_avg:424.40ms
step:1357/1500 train_loss:3.7288 train_time:571656ms step_avg:424.39ms
step:1358/1500 train_loss:3.6684 train_time:572072ms step_avg:424.39ms
step:1359/1500 train_loss:3.3809 train_time:572489ms step_avg:424.38ms
step:1360/1500 train_loss:3.6605 train_time:572906ms step_avg:424.37ms
step:1361/1500 train_loss:3.5406 train_time:573322ms step_avg:424.37ms
step:1362/1500 train_loss:3.3826 train_time:573739ms step_avg:424.36ms
step:1363/1500 train_loss:3.5870 train_time:574155ms step_avg:424.36ms
step:1364/1500 train_loss:3.4758 train_time:574572ms step_avg:424.35ms
step:1365/1500 train_loss:3.4939 train_time:574991ms step_avg:424.35ms
step:1366/1500 train_loss:3.5205 train_time:575407ms step_avg:424.34ms
step:1367/1500 train_loss:3.6193 train_time:575823ms step_avg:424.34ms
step:1368/1500 train_loss:3.6100 train_time:576238ms step_avg:424.33ms
step:1369/1500 train_loss:3.5514 train_time:576657ms step_avg:424.32ms
step:1370/1500 train_loss:3.4750 train_time:577072ms step_avg:424.32ms
step:1371/1500 train_loss:3.7951 train_time:577488ms step_avg:424.31ms
step:1372/1500 train_loss:3.5324 train_time:577907ms step_avg:424.31ms
step:1373/1500 train_loss:3.5744 train_time:578324ms step_avg:424.30ms
step:1374/1500 train_loss:3.5690 train_time:578745ms step_avg:424.30ms
step:1375/1500 train_loss:3.3671 train_time:579161ms step_avg:424.29ms
step:1375/1500 val_loss:3.5243 train_time:579175ms step_avg:424.30ms
step:1376/1500 train_loss:3.7574 train_time:579581ms step_avg:424.29ms
step:1377/1500 train_loss:3.5519 train_time:579998ms step_avg:424.29ms
step:1378/1500 train_loss:3.6881 train_time:580413ms step_avg:424.28ms
step:1379/1500 train_loss:3.7183 train_time:580831ms step_avg:424.27ms
step:1380/1500 train_loss:3.3532 train_time:581246ms step_avg:424.27ms
step:1381/1500 train_loss:3.5268 train_time:581662ms step_avg:424.26ms
step:1382/1500 train_loss:3.9571 train_time:582078ms step_avg:424.26ms
step:1383/1500 train_loss:3.4414 train_time:582494ms step_avg:424.25ms
step:1384/1500 train_loss:3.6023 train_time:582912ms step_avg:424.24ms
step:1385/1500 train_loss:3.6748 train_time:583328ms step_avg:424.24ms
step:1386/1500 train_loss:3.5891 train_time:583745ms step_avg:424.23ms
step:1387/1500 train_loss:3.5692 train_time:584163ms step_avg:424.23ms
step:1388/1500 train_loss:3.4134 train_time:584579ms step_avg:424.22ms
step:1389/1500 train_loss:3.5473 train_time:584996ms step_avg:424.22ms
step:1390/1500 train_loss:3.5253 train_time:585411ms step_avg:424.21ms
step:1391/1500 train_loss:3.7856 train_time:585828ms step_avg:424.21ms
step:1392/1500 train_loss:3.5025 train_time:586244ms step_avg:424.20ms
step:1393/1500 train_loss:3.4964 train_time:586663ms step_avg:424.20ms
step:1394/1500 train_loss:3.4559 train_time:587079ms step_avg:424.19ms
step:1395/1500 train_loss:3.7363 train_time:587495ms step_avg:424.18ms
step:1396/1500 train_loss:3.6351 train_time:587910ms step_avg:424.18ms
step:1397/1500 train_loss:3.6396 train_time:588328ms step_avg:424.17ms
step:1398/1500 train_loss:3.5100 train_time:588744ms step_avg:424.17ms
step:1399/1500 train_loss:3.4813 train_time:589163ms step_avg:424.16ms
step:1400/1500 train_loss:3.5411 train_time:589582ms step_avg:424.16ms
step:1401/1500 train_loss:3.5183 train_time:589998ms step_avg:424.15ms
step:1402/1500 train_loss:3.5485 train_time:590416ms step_avg:424.15ms
step:1403/1500 train_loss:3.5097 train_time:590832ms step_avg:424.14ms
step:1404/1500 train_loss:3.7398 train_time:591248ms step_avg:424.14ms
step:1405/1500 train_loss:3.4839 train_time:591667ms step_avg:424.13ms
step:1406/1500 train_loss:3.5302 train_time:592083ms step_avg:424.13ms
step:1407/1500 train_loss:3.5242 train_time:592500ms step_avg:424.12ms
step:1408/1500 train_loss:3.3901 train_time:592916ms step_avg:424.12ms
step:1409/1500 train_loss:3.5171 train_time:593340ms step_avg:424.12ms
step:1410/1500 train_loss:3.4967 train_time:593760ms step_avg:424.11ms
step:1411/1500 train_loss:3.4946 train_time:594177ms step_avg:424.11ms
step:1412/1500 train_loss:3.5818 train_time:594593ms step_avg:424.10ms
step:1413/1500 train_loss:3.5202 train_time:595009ms step_avg:424.10ms
step:1414/1500 train_loss:3.5690 train_time:595426ms step_avg:424.09ms
step:1415/1500 train_loss:3.5527 train_time:595842ms step_avg:424.09ms
step:1416/1500 train_loss:3.6300 train_time:596263ms step_avg:424.08ms
step:1417/1500 train_loss:3.4360 train_time:596679ms step_avg:424.08ms
step:1418/1500 train_loss:3.5037 train_time:597096ms step_avg:424.07ms
step:1419/1500 train_loss:3.5932 train_time:597511ms step_avg:424.07ms
step:1420/1500 train_loss:3.6219 train_time:597927ms step_avg:424.06ms
step:1421/1500 train_loss:3.6007 train_time:598345ms step_avg:424.06ms
step:1422/1500 train_loss:3.5804 train_time:598763ms step_avg:424.05ms
step:1423/1500 train_loss:3.5544 train_time:599181ms step_avg:424.05ms
step:1424/1500 train_loss:3.5527 train_time:599598ms step_avg:424.04ms
step:1425/1500 train_loss:3.5585 train_time:600013ms step_avg:424.04ms
step:1426/1500 train_loss:3.4256 train_time:600429ms step_avg:424.03ms
step:1427/1500 train_loss:3.5306 train_time:600844ms step_avg:424.03ms
step:1428/1500 train_loss:3.4830 train_time:601262ms step_avg:424.02ms
step:1429/1500 train_loss:3.5913 train_time:601677ms step_avg:424.01ms
step:1430/1500 train_loss:3.5606 train_time:602093ms step_avg:424.01ms
step:1431/1500 train_loss:3.4837 train_time:602511ms step_avg:424.00ms
step:1432/1500 train_loss:3.5329 train_time:602928ms step_avg:424.00ms
step:1433/1500 train_loss:3.5683 train_time:603344ms step_avg:423.99ms
step:1434/1500 train_loss:3.3822 train_time:603764ms step_avg:423.99ms
step:1435/1500 train_loss:3.5369 train_time:604182ms step_avg:423.99ms
step:1436/1500 train_loss:3.3528 train_time:604599ms step_avg:423.98ms
step:1437/1500 train_loss:3.4333 train_time:605015ms step_avg:423.98ms
step:1438/1500 train_loss:3.6244 train_time:605431ms step_avg:423.97ms
step:1439/1500 train_loss:3.5862 train_time:605847ms step_avg:423.97ms
step:1440/1500 train_loss:3.5311 train_time:606282ms step_avg:423.97ms
step:1441/1500 train_loss:3.3926 train_time:606699ms step_avg:423.97ms
step:1442/1500 train_loss:3.5634 train_time:607115ms step_avg:423.96ms
step:1443/1500 train_loss:3.6181 train_time:607530ms step_avg:423.96ms
step:1444/1500 train_loss:3.7039 train_time:607946ms step_avg:423.95ms
step:1445/1500 train_loss:3.6618 train_time:608364ms step_avg:423.95ms
step:1446/1500 train_loss:3.5478 train_time:608780ms step_avg:423.94ms
step:1447/1500 train_loss:3.4197 train_time:609198ms step_avg:423.94ms
step:1448/1500 train_loss:3.4963 train_time:609614ms step_avg:423.93ms
step:1449/1500 train_loss:3.5155 train_time:610029ms step_avg:423.93ms
step:1450/1500 train_loss:3.6300 train_time:610445ms step_avg:423.92ms
step:1451/1500 train_loss:3.6210 train_time:610863ms step_avg:423.92ms
step:1452/1500 train_loss:3.4374 train_time:611278ms step_avg:423.91ms
step:1453/1500 train_loss:3.5565 train_time:611694ms step_avg:423.90ms
step:1454/1500 train_loss:3.4702 train_time:612111ms step_avg:423.90ms
step:1455/1500 train_loss:3.5016 train_time:612527ms step_avg:423.89ms
step:1456/1500 train_loss:3.5489 train_time:612942ms step_avg:423.89ms
step:1457/1500 train_loss:3.4826 train_time:613363ms step_avg:423.89ms
step:1458/1500 train_loss:3.3732 train_time:613780ms step_avg:423.88ms
step:1459/1500 train_loss:3.6198 train_time:614195ms step_avg:423.88ms
step:1460/1500 train_loss:3.4902 train_time:614612ms step_avg:423.87ms
step:1461/1500 train_loss:3.5437 train_time:615028ms step_avg:423.87ms
step:1462/1500 train_loss:3.6648 train_time:615444ms step_avg:423.86ms
step:1463/1500 train_loss:3.4825 train_time:615863ms step_avg:423.86ms
step:1464/1500 train_loss:3.6768 train_time:616281ms step_avg:423.85ms
step:1465/1500 train_loss:3.5699 train_time:616696ms step_avg:423.85ms
step:1466/1500 train_loss:3.5737 train_time:617112ms step_avg:423.84ms
step:1467/1500 train_loss:3.4968 train_time:617530ms step_avg:423.84ms
step:1468/1500 train_loss:3.6510 train_time:617944ms step_avg:423.83ms
step:1469/1500 train_loss:3.5227 train_time:618364ms step_avg:423.83ms
step:1470/1500 train_loss:3.4911 train_time:618781ms step_avg:423.82ms
step:1471/1500 train_loss:3.5442 train_time:619196ms step_avg:423.82ms
step:1472/1500 train_loss:3.4683 train_time:619614ms step_avg:423.81ms
step:1473/1500 train_loss:3.5739 train_time:620029ms step_avg:423.81ms
step:1474/1500 train_loss:3.6477 train_time:620445ms step_avg:423.80ms
step:1475/1500 train_loss:3.5275 train_time:620864ms step_avg:423.80ms
step:1476/1500 train_loss:3.3621 train_time:621280ms step_avg:423.79ms
step:1477/1500 train_loss:3.4765 train_time:621697ms step_avg:423.79ms
step:1478/1500 train_loss:3.4463 train_time:622113ms step_avg:423.78ms
step:1479/1500 train_loss:3.5406 train_time:622529ms step_avg:423.78ms
step:1480/1500 train_loss:3.6203 train_time:622946ms step_avg:423.77ms
step:1481/1500 train_loss:3.4818 train_time:623363ms step_avg:423.77ms
step:1482/1500 train_loss:3.6635 train_time:623781ms step_avg:423.76ms
step:1483/1500 train_loss:3.5981 train_time:624196ms step_avg:423.76ms
step:1484/1500 train_loss:3.4932 train_time:624612ms step_avg:423.75ms
step:1485/1500 train_loss:3.4759 train_time:625028ms step_avg:423.75ms
step:1486/1500 train_loss:3.4836 train_time:625444ms step_avg:423.74ms
step:1487/1500 train_loss:3.4574 train_time:625864ms step_avg:423.74ms
step:1488/1500 train_loss:3.5415 train_time:626280ms step_avg:423.73ms
step:1489/1500 train_loss:3.4645 train_time:626696ms step_avg:423.73ms
step:1490/1500 train_loss:3.5451 train_time:627114ms step_avg:423.73ms
step:1491/1500 train_loss:3.4745 train_time:627529ms step_avg:423.72ms
step:1492/1500 train_loss:3.4031 train_time:627945ms step_avg:423.71ms
step:1493/1500 train_loss:3.4794 train_time:628363ms step_avg:423.71ms
step:1494/1500 train_loss:3.6550 train_time:628780ms step_avg:423.71ms
step:1495/1500 train_loss:3.5071 train_time:629198ms step_avg:423.70ms
step:1496/1500 train_loss:3.2617 train_time:629614ms step_avg:423.70ms
step:1497/1500 train_loss:3.5731 train_time:630030ms step_avg:423.69ms
step:1498/1500 train_loss:3.5324 train_time:630447ms step_avg:423.69ms
step:1499/1500 train_loss:3.5786 train_time:630864ms step_avg:423.68ms
step:1500/1500 train_loss:3.5329 train_time:631281ms step_avg:423.68ms
step:1500/1500 val_loss:3.5091 train_time:631295ms step_avg:423.69ms
